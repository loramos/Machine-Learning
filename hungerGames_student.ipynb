{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KX8zKrS-hRJk",
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Práctica: Los Juegos del Hambre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XysGzsSAmBvu"
   },
   "source": [
    "ESTUDIANTE: LORENZO OTERO RAMOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vanYztAMhRJt",
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "<table><tr>\n",
    "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/breakfast.jpg\" style=\"width:300px;height:300px;\"></td>\n",
    "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/hamburger.jpg\" style=\"width:300px;height:300px;\"></td>\n",
    "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/fruits.jpg\" style=\"width:300px;height:300px;\"></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6E7BW750hRJu",
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "En esta práctica vamos a enfrentarnos a un problema desafiante de clasificación de imágenes, construyendo una red neuronal profunda que sea capaz de clasificar entre diferentes tipos de comida. ¡Que comiencen los Juegos del Hambre!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mnn_FAm8hRJv",
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "## Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gspUM6n3hRJw"
   },
   "source": [
    "A lo largo del notebook encontrarás celdas que debes rellenar con tu propio código. Sigue las instrucciones del notebook y presta atención a los siguientes iconos:\n",
    "\n",
    "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
    "\n",
    "***\n",
    "\n",
    "<font color=#ad3e26>\n",
    "Deberás resolver el ejercicio escribiendo tu propio código o respuesta en la celda inmediatamente inferior.</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZHQpQXrhRJw"
   },
   "source": [
    "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
    "\n",
    "***\n",
    "<font color=#2655ad>\n",
    "Esto es una pista u observación de utilidad que puede ayudarte a resolver el ejercicio. Presta atención a estas pistas para comprender el ejercicio en mayor profundidad.\n",
    "</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xghhJf_HhRJx"
   },
   "source": [
    "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
    "\n",
    "***\n",
    "<font color=#259b4c>\n",
    "Este es un ejercicio avanzado que te puede ayudar a profundizar en el tema, y a conseguir una calificación más alta. ¡Buena suerte!</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWAbqrofhRJy"
   },
   "source": [
    "Para evitar problemas con imports o incompatibilidades se recomienda ejecutar este notebook en uno de los [entornos de Deep Learning recomendados](https://github.com/albarji/teaching-environments-deeplearning), o hacer uso [Google Colaboratory](https://colab.research.google.com/). Si usas Colaboratory, asegúrate de [conectar una GPU](https://colab.research.google.com/notebooks/gpu.ipynb), y de haber [deactivado otras sesiones que tuvieras activas](https://stackoverflow.com/a/53441194/2436578)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZmf8R9DhRJy"
   },
   "source": [
    "El siguiente código mostrará todas las gráficas en el propio notebook en lugar de generar una nueva ventana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DxmdgWqNhRJz",
    "outputId": "1b39cec5-457a-4bdd-98b3-b5f20fdfb044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "!pwd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RUEFj1cDpGGA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UgDu1WUhRJ0"
   },
   "source": [
    "Finalmente, si necesitas ayuda en el uso de cualquier función Python, coloca el cursor sobre su nombre y presiona Shift+Tab. Aparecerá una ventana con su documentación. Esto solo funciona dentro de celdas de código.\n",
    "\n",
    "¡Vamos alla!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K733uf0ghRJ2"
   },
   "source": [
    "## Obtención de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZYPquwFhRJ2"
   },
   "source": [
    "Vamos a usar un dataset de imágenes de comida disponible en [Kaggle](https://www.kaggle.com/trolukovich/food11-image-dataset). Para descargarlo, necesitarás crear una cuenta de usuario en Kaggle, y obtener tus credenciales de la API. Puedes hacerlo siguiendo las instrucciones de [esta sección](https://github.com/Kaggle/kaggle-api#api-credentials). ¡Ojo! Tus credenciales de la API no son lo mismo que la contraseña que utilizas para acceder a tu cuenta en Kaggle.\n",
    "\n",
    "Una vez tengas el fichero JSON con tus credenciales, puedes declararlas en este notebook asignando las variables de entorno adecuadas, de la siguiente manera\n",
    "\n",
    "    import os\n",
    "\n",
    "    os.environ[\"KAGGLE_USERNAME\"] = \"YOUR KAGGLE USERNAME HERE\"\n",
    "    os.environ[\"KAGGLE_KEY\"] = \"YOUR KAGGLE KEY HERE\"\n",
    "    \n",
    "Cuando lo hayas hecho, podrás descargar el dataset a la máquina donde esté corriendo este notebook usando el siguiente comando\n",
    "\n",
    "    !kaggle datasets download trolukovich/food11-image-dataset --unzip -p YOUR_LOCAL_FOLDER\n",
    "    \n",
    "donde debes indicar el nombre de un directorio válido como \"YOUR_LOCAL_FOLDER\". Si prefieres descargar los datos en la misma carpeta que este notebook, puedes quitar la parte `-p YOUR_LOCAL_FOLDER` del comando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRX8BQsZhRJ2"
   },
   "source": [
    "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
    "\n",
    "***\n",
    "\n",
    "<font color=#ad3e26>\n",
    "Crea tu propia cuenta de Kaggle (si no tienes ya una), obtén tus credenciales, y usa la celda inferior para declarar tu nombre de usuario de Kaggle y tu clave como variables de entorno. A continuación, usa la misma celda para descargar el dataset de imágenes.\n",
    "    \n",
    "¡Ojo! Debes mantener estas credenciales en secreto, ya que son personales a tu usuario de Kaggle. Recuerda borrarlas de la celda antes de entregar este notebook.\n",
    "</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wiFYSULfhRJ3",
    "outputId": "b55672ae-5216-4e1d-f6dd-e72ee72850b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading food11-image-dataset.zip to data\n",
      "100% 1.08G/1.08G [00:36<00:00, 34.9MB/s]\n",
      "100% 1.08G/1.08G [00:36<00:00, 32.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KAGGLE_USERNAME\"] = \"lorenzooteroramos\"\n",
    "os.environ[\"KAGGLE_KEY\"] = \"\"\n",
    "\n",
    "!kaggle datasets download trolukovich/food11-image-dataset --unzip -p data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xmh27gIEhRJ4"
   },
   "source": [
    "Revisa ahora la carpeta en la que has descargado los datos. Verás que contiene 3 subdirectorios:\n",
    "\n",
    "* **training**, contiene las imágenes a utilizar para entrenar el modelo.\n",
    "* **validation**, contiene imágenes adicionales que podrías usar como datos de entrenamiento adicionales, o para algún tipo de estrategia de validación como Early Stopping.\n",
    "* **evaluation**, contiene las imágenes que debes utilizar para testear el modelo. Las imágenes de esta carpeta **solo** pueden utilizarse para medir el rendimiento del modelo tras su entrenamiento, y para nada más.\n",
    "\n",
    "Además de esto, dentro de cada una de estas carpetas encontrarás una subcarpeta para cada una de las 11 clases de comida:\n",
    "\n",
    "* Bread (panes)\n",
    "* Dairy product (lácteos)\n",
    "* Dessert (postres)\n",
    "* Egg (huevos)\n",
    "* Fried food (fritos)\n",
    "* Meat (carnes)\n",
    "* Noodles-Pasta (pasta)\n",
    "* Rice (arroz)\n",
    "* Seafood (pescado y marisco)\n",
    "* Soup (sopas)\n",
    "* Vegetable-Fruit (vegetales y frutas)\n",
    "\n",
    "Esta es una forma estándar de organizar los datasets de imágenes: una carpeta para cada clase. Para facilitar los pasos de procesamiento que vendrán a continuación, vamos a definir algunas variables que nos indiquen dónde están almacenados los diferentes conjuntos de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJrE8F8zhRJ4"
   },
   "source": [
    "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
    "\n",
    "***\n",
    "\n",
    "<font color=#ad3e26>\n",
    "    Crea variables <b>TRAINDIR</b>, <b>VALDIR</b> y <b>TESTDIR</b>, cada una conteniendo una cadena de texto con la ruta al directorio donde están los datos de entrenamiento, validación y evaluación, respectivamente.\n",
    "</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TzK2VI3ShRJ5"
   },
   "outputs": [],
   "source": [
    "TRAINDIR = \"/content/data/training\"\n",
    "VALDIR = \"/content/data/validation\"\n",
    "TESTDIR = \"/content/data/evaluation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikYtQyR-hRJ6"
   },
   "source": [
    "### Reducción de clases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWsLot-7hRJ6"
   },
   "source": [
    "Con el fin de hacer este problema más accesible de cara a la práctica, vamos a centrarnos solo en seis de las clases de comida disponibles: `Bread`, `Dairy product`, `Dessert`, `Egg`, `Fried food` y `Meat`. Para ello, se provee el código siguiente, que elimina de los datos descargados las carpetas correspondientes a imágenes de las otras clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TXoNw1-thRJ7",
    "outputId": "32ed95f2-1cfb-4234-9a9b-58e1a2af7bf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting /content/data/training/Soup...\n",
      "Deleting /content/data/training/Noodles-Pasta...\n",
      "Deleting /content/data/training/Seafood...\n",
      "Deleting /content/data/training/Vegetable-Fruit...\n",
      "Deleting /content/data/training/Rice...\n",
      "Deleting /content/data/evaluation/Soup...\n",
      "Deleting /content/data/evaluation/Noodles-Pasta...\n",
      "Deleting /content/data/evaluation/Seafood...\n",
      "Deleting /content/data/evaluation/Vegetable-Fruit...\n",
      "Deleting /content/data/evaluation/Rice...\n",
      "Deleting /content/data/validation/Soup...\n",
      "Deleting /content/data/validation/Noodles-Pasta...\n",
      "Deleting /content/data/validation/Seafood...\n",
      "Deleting /content/data/validation/Vegetable-Fruit...\n",
      "Deleting /content/data/validation/Rice...\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "valid_classes = {\"Bread\", \"Dairy product\", \"Dessert\", \"Egg\", \"Fried food\", \"Meat\"}\n",
    "datasets = {TRAINDIR, VALDIR, TESTDIR}\n",
    "\n",
    "for dataset in datasets:\n",
    "    for classdir in glob(f\"{dataset}/*\"):  # Find subfolders with classes\n",
    "        if classdir.split(\"/\")[-1] not in valid_classes:  # Ignore those in valid_classes\n",
    "            print(f\"Deleting {classdir}...\")\n",
    "            for fname in glob(f\"{classdir}/*.jpg\"):  # Remove each image file\n",
    "                os.remove(fname)\n",
    "            os.rmdir(classdir)  # Remove folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fncCfOYihRJ7"
   },
   "source": [
    "## Procesando imágenes desde ficheros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wTWrPr-hRJ7"
   },
   "source": [
    "Este dataset de imágenes es grande, con imágenes de mayor resolución que las que hemos utilizado en el tutorial del MNIST, y cada una de ellas teniendo diferentes tamaños y relación de aspecto. Además, mientras que para el MNIST teníamos una función de keras que preparaba los datos para nosotros, en esta ocasión tendremos que realizar el trabajo de carga y procesamiento de las imágenes.\n",
    "\n",
    "Una forma conveniente de hacer todo este trabajo es a través de la función Keras `image_dataset_from_directory`. Esta función crea un objeto `Dataset` de TensorFlow con todas las imágenes de un directorio, cargándolas en memoria de forma dinámica solo cuando la red neuronal necesita utilizarlas. Esta función también nos permite especificar algunas opciones de preprocesamiento muy útiles.\n",
    "\n",
    "Por ejemplo, podemos crear un `Dataset` con los datos en la carpeta de training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-edpP7-NhRJ8",
    "outputId": "fddfd5cd-e6e9-4e34-f94a-24c0a7e22380"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6082 files belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "image_size = 32\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    TRAINDIR, \n",
    "    image_size = (image_size, image_size),\n",
    "    batch_size = batch_size, \n",
    "    label_mode = 'categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGEdEuF5hRJ8"
   },
   "source": [
    "Observa los parámetros que se han utilizado para configurar el dataset:\n",
    "\n",
    "* El **directorio** desde el que cargar las imágenes.\n",
    "* Un **tamaño de imagen (image_size)** que se utilizará para redimensionar todas las imágenes cargadas a ese tamaño común, en este caso 32x32 píxeles.\n",
    "* El **tamaños de los lotes (batch_size)** de imágenes a ser generados. Nótese que definimos aquí este parametro en lugar de en el paso `fit` de la red, como hemos hecho en otros ejercicios, porque el objeto `Dataset` resultante hará uso de esta información para mantener en memoria solo algunos batches de imágenes, ahorrando así memoria.\n",
    "* El **modo de etiquetado (label_mode)**, esto es, la codificación de las etiquetas a utilizar. `categorical` significa que utilizaremos la ya conocida codificación one-hot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgeGjQDVhRJ9"
   },
   "source": [
    "Un objeto `Dataset` funciona de manera muy similar a un generador de Python, lo que significa que podemos iterar sobre él para obtener batches de imágenes ya preprocesadas. Por ejemplo, el siguiente código inicia un bucle para extraer todos los batches del `Dataset`, nos muestra el contenido el primero, y detiene la iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bPAIrHu7hRJ9",
    "outputId": "cecdcb5d-f9b4-44ee-f075-04082b643d91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input batch: (64, 32, 32, 3)\n",
      "Shape of output batch: (64, 6)\n",
      "Input batch:\n",
      "[[[[1.27289062e+02 1.22289062e+02 1.18789062e+02]\n",
      "   [1.33929688e+02 1.31429688e+02 1.29929688e+02]\n",
      "   [1.39929688e+02 1.34929688e+02 1.31429688e+02]\n",
      "   ...\n",
      "   [1.02929688e+02 7.00703125e+01 5.10000000e+01]\n",
      "   [1.20710938e+02 9.27109375e+01 7.87109375e+01]\n",
      "   [1.42359375e+02 1.19359375e+02 1.05359375e+02]]\n",
      "\n",
      "  [[1.33132812e+02 1.29132812e+02 1.26132812e+02]\n",
      "   [1.38289062e+02 1.33789062e+02 1.30789062e+02]\n",
      "   [1.42789062e+02 1.37789062e+02 1.34789062e+02]\n",
      "   ...\n",
      "   [1.06398438e+02 7.10000000e+01 5.01328125e+01]\n",
      "   [9.13671875e+01 6.04765625e+01 4.43906250e+01]\n",
      "   [1.22867188e+02 9.91328125e+01 8.72656250e+01]]\n",
      "\n",
      "  [[1.37648438e+02 1.32648438e+02 1.29648438e+02]\n",
      "   [1.41648438e+02 1.37148438e+02 1.34148438e+02]\n",
      "   [1.45703125e+02 1.37703125e+02 1.34703125e+02]\n",
      "   ...\n",
      "   [8.85546875e+01 6.45000000e+01 4.74453125e+01]\n",
      "   [4.89062500e+01 3.12031250e+01 2.15000000e+01]\n",
      "   [8.58906250e+01 6.13515625e+01 4.82031250e+01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.26351562e+02 1.12351562e+02 1.12351562e+02]\n",
      "   [1.89054688e+02 2.10648438e+02 2.45351562e+02]\n",
      "   [1.71648438e+02 1.96648438e+02 2.26648438e+02]\n",
      "   ...\n",
      "   [1.64851562e+02 1.65703125e+02 1.71148438e+02]\n",
      "   [1.61500000e+02 1.62500000e+02 1.67500000e+02]\n",
      "   [1.59796875e+02 1.58796875e+02 1.64796875e+02]]\n",
      "\n",
      "  [[1.29812500e+02 1.10812500e+02 1.06390625e+02]\n",
      "   [2.13843750e+02 2.29843750e+02 2.54421875e+02]\n",
      "   [1.71289062e+02 1.94289062e+02 2.25289062e+02]\n",
      "   ...\n",
      "   [1.62210938e+02 1.63210938e+02 1.68210938e+02]\n",
      "   [1.60210938e+02 1.59210938e+02 1.65210938e+02]\n",
      "   [1.56789062e+02 1.55789062e+02 1.61789062e+02]]\n",
      "\n",
      "  [[1.31500000e+02 1.11359375e+02 1.12929688e+02]\n",
      "   [1.48375000e+02 1.11656250e+02 1.06210938e+02]\n",
      "   [1.80070312e+02 2.00070312e+02 2.34210938e+02]\n",
      "   ...\n",
      "   [1.61500000e+02 1.58500000e+02 1.65500000e+02]\n",
      "   [1.58500000e+02 1.57500000e+02 1.63500000e+02]\n",
      "   [1.57570312e+02 1.55570312e+02 1.60570312e+02]]]\n",
      "\n",
      "\n",
      " [[[2.85000000e+01 1.70000000e+01 1.60000000e+01]\n",
      "   [2.97500000e+01 1.97500000e+01 1.82500000e+01]\n",
      "   [3.07500000e+01 1.87500000e+01 1.87500000e+01]\n",
      "   ...\n",
      "   [1.07250000e+02 1.61000000e+02 2.16250000e+02]\n",
      "   [1.17750000e+02 1.85250000e+02 2.43750000e+02]\n",
      "   [1.45750000e+02 2.18000000e+02 2.53750000e+02]]\n",
      "\n",
      "  [[3.82500000e+01 2.70000000e+01 2.62500000e+01]\n",
      "   [3.37500000e+01 2.17500000e+01 2.22500000e+01]\n",
      "   [3.47500000e+01 2.22500000e+01 2.07500000e+01]\n",
      "   ...\n",
      "   [9.45000000e+01 1.33500000e+02 1.79500000e+02]\n",
      "   [9.37500000e+01 1.53250000e+02 2.17750000e+02]\n",
      "   [4.87500000e+01 9.47500000e+01 1.38250000e+02]]\n",
      "\n",
      "  [[3.02500000e+01 2.27500000e+01 2.32500000e+01]\n",
      "   [3.55000000e+01 2.65000000e+01 2.75000000e+01]\n",
      "   [4.15000000e+01 2.70000000e+01 2.40000000e+01]\n",
      "   ...\n",
      "   [3.72500000e+01 4.05000000e+01 5.47500000e+01]\n",
      "   [3.30000000e+01 5.00000000e+01 7.90000000e+01]\n",
      "   [3.67500000e+01 5.75000000e+01 9.67500000e+01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[6.52500000e+01 8.37500000e+01 1.14250000e+02]\n",
      "   [5.85000000e+01 7.65000000e+01 1.00250000e+02]\n",
      "   [1.04750000e+02 1.05500000e+02 8.12500000e+01]\n",
      "   ...\n",
      "   [1.44500000e+02 1.87500000e+02 2.31500000e+02]\n",
      "   [1.45750000e+02 1.90500000e+02 2.33250000e+02]\n",
      "   [1.44000000e+02 1.84500000e+02 2.37500000e+02]]\n",
      "\n",
      "  [[7.52500000e+01 1.04750000e+02 1.44500000e+02]\n",
      "   [6.12500000e+01 8.45000000e+01 1.17500000e+02]\n",
      "   [5.75000000e+01 7.65000000e+01 1.07500000e+02]\n",
      "   ...\n",
      "   [1.43000000e+02 1.83500000e+02 2.25000000e+02]\n",
      "   [1.35500000e+02 1.85000000e+02 2.32500000e+02]\n",
      "   [1.34750000e+02 1.84250000e+02 2.33000000e+02]]\n",
      "\n",
      "  [[1.26500000e+02 1.68500000e+02 2.14000000e+02]\n",
      "   [1.04000000e+02 1.36750000e+02 1.81000000e+02]\n",
      "   [7.30000000e+01 1.01500000e+02 1.48750000e+02]\n",
      "   ...\n",
      "   [1.47000000e+02 1.87250000e+02 2.31500000e+02]\n",
      "   [1.44750000e+02 1.89250000e+02 2.31500000e+02]\n",
      "   [1.54750000e+02 2.03750000e+02 2.44000000e+02]]]\n",
      "\n",
      "\n",
      " [[[2.23750000e+02 1.53250000e+02 7.70000000e+01]\n",
      "   [2.24750000e+02 1.54250000e+02 6.62500000e+01]\n",
      "   [2.39250000e+02 1.72750000e+02 9.02500000e+01]\n",
      "   ...\n",
      "   [1.86000000e+02 1.05750000e+02 4.70000000e+01]\n",
      "   [1.67500000e+02 8.85000000e+01 4.25000000e+01]\n",
      "   [1.61500000e+02 8.55000000e+01 4.20000000e+01]]\n",
      "\n",
      "  [[1.87250000e+02 1.08250000e+02 3.32500000e+01]\n",
      "   [1.89500000e+02 1.07500000e+02 3.40000000e+01]\n",
      "   [2.17750000e+02 1.40750000e+02 6.07500000e+01]\n",
      "   ...\n",
      "   [1.35750000e+02 5.62500000e+01 2.30000000e+01]\n",
      "   [7.50000000e+01 1.40000000e+01 0.00000000e+00]\n",
      "   [8.62500000e+01 2.32500000e+01 5.75000000e+00]]\n",
      "\n",
      "  [[7.37500000e+01 4.30000000e+01 1.72500000e+01]\n",
      "   [1.79000000e+02 1.21250000e+02 6.02500000e+01]\n",
      "   [2.24250000e+02 1.46500000e+02 7.10000000e+01]\n",
      "   ...\n",
      "   [1.58500000e+02 7.75000000e+01 2.15000000e+01]\n",
      "   [1.55500000e+02 7.55000000e+01 1.35000000e+01]\n",
      "   [1.56000000e+02 7.22500000e+01 2.00000000e+01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.70000000e+02 1.19000000e+02 5.72500000e+01]\n",
      "   [1.83750000e+02 1.27750000e+02 6.67500000e+01]\n",
      "   [1.77000000e+02 1.25000000e+02 6.60000000e+01]\n",
      "   ...\n",
      "   [1.13500000e+02 4.75000000e+01 2.55000000e+01]\n",
      "   [1.13500000e+02 4.92500000e+01 2.05000000e+01]\n",
      "   [1.13500000e+02 4.85000000e+01 2.05000000e+01]]\n",
      "\n",
      "  [[1.72000000e+02 1.21000000e+02 5.60000000e+01]\n",
      "   [1.90000000e+02 1.36750000e+02 7.07500000e+01]\n",
      "   [1.86250000e+02 1.36250000e+02 6.72500000e+01]\n",
      "   ...\n",
      "   [1.19500000e+02 5.35000000e+01 1.97500000e+01]\n",
      "   [1.28750000e+02 6.07500000e+01 2.27500000e+01]\n",
      "   [1.22250000e+02 5.72500000e+01 2.07500000e+01]]\n",
      "\n",
      "  [[1.46000000e+02 9.20000000e+01 3.80000000e+01]\n",
      "   [1.69000000e+02 1.14750000e+02 5.50000000e+01]\n",
      "   [1.85000000e+02 1.33000000e+02 7.65000000e+01]\n",
      "   ...\n",
      "   [1.15500000e+02 5.05000000e+01 2.25000000e+01]\n",
      "   [1.15250000e+02 5.02500000e+01 1.87500000e+01]\n",
      "   [1.19750000e+02 4.77500000e+01 2.60000000e+01]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[1.27343750e+02 8.60468750e+01 3.53125000e+01]\n",
      "   [1.67828125e+02 1.49234375e+02 9.67343750e+01]\n",
      "   [1.85328125e+02 1.77828125e+02 1.31328125e+02]\n",
      "   ...\n",
      "   [1.03171875e+02 5.32031250e+01 5.05468750e+01]\n",
      "   [9.07968750e+01 4.97968750e+01 4.69843750e+01]\n",
      "   [8.62343750e+01 4.65000000e+01 4.75000000e+01]]\n",
      "\n",
      "  [[1.54703125e+02 1.41437500e+02 9.09375000e+01]\n",
      "   [1.64406250e+02 1.48500000e+02 1.01593750e+02]\n",
      "   [1.54484375e+02 1.36968750e+02 8.41406250e+01]\n",
      "   ...\n",
      "   [1.07656250e+02 5.74843750e+01 4.96875000e+01]\n",
      "   [1.06609375e+02 5.84062500e+01 5.10937500e+01]\n",
      "   [9.25000000e+01 5.05625000e+01 4.56250000e+01]]\n",
      "\n",
      "  [[1.41468750e+02 1.27468750e+02 8.14687500e+01]\n",
      "   [1.43515625e+02 1.31015625e+02 8.45156250e+01]\n",
      "   [1.43187500e+02 1.30687500e+02 8.41875000e+01]\n",
      "   ...\n",
      "   [1.94437500e+02 1.95640625e+02 2.18296875e+02]\n",
      "   [1.64656250e+02 1.68156250e+02 1.95656250e+02]\n",
      "   [1.99421875e+02 2.09656250e+02 2.39218750e+02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[2.05531250e+02 2.05000000e+02 2.02468750e+02]\n",
      "   [2.08000000e+02 2.08000000e+02 2.06000000e+02]\n",
      "   [2.08500000e+02 2.08500000e+02 2.10500000e+02]\n",
      "   ...\n",
      "   [1.77359375e+02 1.61828125e+02 5.91718750e+01]\n",
      "   [1.62015625e+02 1.42203125e+02 2.37968750e+01]\n",
      "   [1.60796875e+02 1.44328125e+02 2.80625000e+01]]\n",
      "\n",
      "  [[2.01765625e+02 2.01234375e+02 1.98703125e+02]\n",
      "   [2.03296875e+02 2.03296875e+02 2.03296875e+02]\n",
      "   [2.03000000e+02 2.03000000e+02 2.03000000e+02]\n",
      "   ...\n",
      "   [1.32250000e+02 1.17406250e+02 2.70937500e+01]\n",
      "   [1.45781250e+02 1.31781250e+02 2.59687500e+01]\n",
      "   [1.33343750e+02 1.13609375e+02 2.16406250e+01]]\n",
      "\n",
      "  [[1.99296875e+02 1.98234375e+02 1.98765625e+02]\n",
      "   [1.98109375e+02 1.99109375e+02 2.01109375e+02]\n",
      "   [2.02484375e+02 2.03484375e+02 2.05484375e+02]\n",
      "   ...\n",
      "   [9.26718750e+01 7.95000000e+01 2.90000000e+01]\n",
      "   [1.01406250e+02 9.04062500e+01 3.09062500e+01]\n",
      "   [1.11875000e+02 1.00875000e+02 4.04062500e+01]]]\n",
      "\n",
      "\n",
      " [[[1.90500000e+02 1.38703125e+02 1.19437500e+02]\n",
      "   [1.96890625e+02 1.46093750e+02 1.23203125e+02]\n",
      "   [1.93656250e+02 1.42484375e+02 1.22484375e+02]\n",
      "   ...\n",
      "   [3.41250000e+01 4.27968750e+01 5.32968750e+01]\n",
      "   [6.00937500e+01 8.18125000e+01 1.39406250e+02]\n",
      "   [2.41562500e+01 6.19218750e+01 1.09687500e+02]]\n",
      "\n",
      "  [[1.91734375e+02 1.41734375e+02 1.25734375e+02]\n",
      "   [1.94406250e+02 1.42406250e+02 1.25906250e+02]\n",
      "   [1.92171875e+02 1.40171875e+02 1.22671875e+02]\n",
      "   ...\n",
      "   [2.98593750e+01 5.00000000e+01 7.45156250e+01]\n",
      "   [2.44531250e+01 5.63437500e+01 9.59218750e+01]\n",
      "   [3.67343750e+01 9.38437500e+01 1.58015625e+02]]\n",
      "\n",
      "  [[1.18734375e+02 8.87343750e+01 1.05734375e+02]\n",
      "   [1.35828125e+02 9.93281250e+01 1.09328125e+02]\n",
      "   [1.84203125e+02 1.32703125e+02 1.25203125e+02]\n",
      "   ...\n",
      "   [3.77656250e+01 4.01406250e+01 4.62812500e+01]\n",
      "   [3.52031250e+01 3.57031250e+01 3.92031250e+01]\n",
      "   [3.70937500e+01 4.25937500e+01 5.20937500e+01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[9.25937500e+01 1.04531250e+02 1.60062500e+02]\n",
      "   [9.42031250e+01 1.10203125e+02 1.65296875e+02]\n",
      "   [9.05000000e+01 1.05500000e+02 1.60500000e+02]\n",
      "   ...\n",
      "   [1.74203125e+02 2.03843750e+02 2.02031250e+02]\n",
      "   [1.39781250e+02 1.27015625e+02 8.27968750e+01]\n",
      "   [7.45468750e+01 3.93437500e+01 2.34375000e-01]]\n",
      "\n",
      "  [[9.37343750e+01 1.01734375e+02 1.43234375e+02]\n",
      "   [9.26875000e+01 9.85000000e+01 1.41312500e+02]\n",
      "   [8.09843750e+01 1.01812500e+02 1.52500000e+02]\n",
      "   ...\n",
      "   [1.67875000e+02 1.74656250e+02 1.01093750e+02]\n",
      "   [1.87640625e+02 2.18734375e+02 2.03781250e+02]\n",
      "   [1.40906250e+02 1.32750000e+02 1.02968750e+02]]\n",
      "\n",
      "  [[1.03125000e+02 8.16250000e+01 9.56250000e+01]\n",
      "   [9.47968750e+01 5.10937500e+01 3.91718750e+01]\n",
      "   [8.29218750e+01 4.15781250e+01 3.54687500e+01]\n",
      "   ...\n",
      "   [1.57000000e+02 1.27656250e+02 3.73437500e+01]\n",
      "   [1.60468750e+02 1.39656250e+02 2.75625000e+01]\n",
      "   [1.85093750e+02 2.09828125e+02 1.92046875e+02]]]\n",
      "\n",
      "\n",
      " [[[1.02968750e+01 2.28515625e+01 3.10546875e+01]\n",
      "   [5.60156250e+00 2.26015625e+01 3.16015625e+01]\n",
      "   [1.31171875e+01 3.46171875e+01 5.46171875e+01]\n",
      "   ...\n",
      "   [7.02734375e+01 6.82734375e+01 5.52734375e+01]\n",
      "   [7.61093750e+01 7.36093750e+01 5.96093750e+01]\n",
      "   [8.20546875e+01 7.95546875e+01 6.55546875e+01]]\n",
      "\n",
      "  [[1.07421875e+01 6.22421875e+01 9.09453125e+01]\n",
      "   [1.61171875e+01 7.15078125e+01 9.01718750e+01]\n",
      "   [2.42187500e-01 5.20156250e+01 8.35156250e+01]\n",
      "   ...\n",
      "   [5.02578125e+01 4.82578125e+01 3.52578125e+01]\n",
      "   [5.45000000e+01 5.15000000e+01 4.25000000e+01]\n",
      "   [7.64140625e+01 7.54140625e+01 5.67109375e+01]]\n",
      "\n",
      "  [[1.59062500e+01 3.50000000e+01 6.19453125e+01]\n",
      "   [2.34531250e+01 4.86718750e+01 7.48437500e+01]\n",
      "   [3.90312500e+01 6.42578125e+01 7.85156250e+01]\n",
      "   ...\n",
      "   [5.60000000e+01 5.30000000e+01 4.60000000e+01]\n",
      "   [5.74453125e+01 5.74453125e+01 4.54453125e+01]\n",
      "   [1.37382812e+02 1.30679688e+02 1.09773438e+02]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.83500000e+02 1.70500000e+02 1.52906250e+02]\n",
      "   [1.81500000e+02 1.69500000e+02 1.47062500e+02]\n",
      "   [1.80742188e+02 1.68742188e+02 1.44742188e+02]\n",
      "   ...\n",
      "   [1.87757812e+02 1.75757812e+02 1.51757812e+02]\n",
      "   [1.90500000e+02 1.75500000e+02 1.52500000e+02]\n",
      "   [1.92500000e+02 1.77500000e+02 1.54500000e+02]]\n",
      "\n",
      "  [[1.79000000e+02 1.68000000e+02 1.46000000e+02]\n",
      "   [1.76000000e+02 1.65000000e+02 1.43000000e+02]\n",
      "   [1.73742188e+02 1.66742188e+02 1.40742188e+02]\n",
      "   ...\n",
      "   [1.87015625e+02 1.72015625e+02 1.50015625e+02]\n",
      "   [1.86500000e+02 1.74500000e+02 1.50500000e+02]\n",
      "   [1.92351562e+02 1.76351562e+02 1.53351562e+02]]\n",
      "\n",
      "  [[1.12742188e+02 1.01390625e+02 1.08648438e+02]\n",
      "   [1.31726562e+02 1.25507812e+02 1.19226562e+02]\n",
      "   [1.56695312e+02 1.47468750e+02 1.30710938e+02]\n",
      "   ...\n",
      "   [1.84000000e+02 1.72000000e+02 1.48000000e+02]\n",
      "   [1.86890625e+02 1.74890625e+02 1.48890625e+02]\n",
      "   [1.87148438e+02 1.75148438e+02 1.49148438e+02]]]]\n",
      "Output batch:\n",
      "[[0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in train_dataset:\n",
    "    print(f\"Shape of input batch: {X_batch.shape}\")\n",
    "    print(f\"Shape of output batch: {y_batch.shape}\")\n",
    "    print(f\"Input batch:\\n{X_batch}\")\n",
    "    print(f\"Output batch:\\n{y_batch}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2UaPCEthRJ-"
   },
   "source": [
    "Podemos ver que, efectivamente, el generador produce un tensor de datos de entrada de las dimensiones apropiadas para poder introducirlo en la red neuronal, y que las salidas también se han codificado correctamente como one-hot.\n",
    "No obstante, todavía hay un problema con los datos: los valores de los píxeles están en el rango [0, 255], lo cual puede producir problemas de entrenamiento. Resolveremos este punto después, en la definición de la red neuronal, mediante una capa especial. Por ahora vamos a cotinuar, definiendo una función que construya los `Dataset` para los datos de entrenamiento, validación y test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTQyBbf-hRJ-"
   },
   "source": [
    "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
    "\n",
    "***\n",
    "\n",
    "<font color=#ad3e26>\n",
    "    Crea una función <b>create_datasets</b> que reciba los siguiente parámetros:\n",
    "    <ul>\n",
    "      <li><b>traindir</b>: el directorio donde están localizadas las imágenes de entrenamiento.</li>\n",
    "      <li><b>valdir</b>: el directorio donde están localizadas las imágenes de validación.</li>\n",
    "      <li><b>testdir</b>: el directorio donde están localizadas las imágenes de test.</li>\n",
    "      <li><b>image_size</b>: el tamaño que se utilizará para redimensionar todas las imágenes a una resolución común.</li>\n",
    "      <li><b>batch_size</b>: el tamaño de los batches de imágenes que serán generados.</li>\n",
    "    </ul>\n",
    "    La función debe crear objetos `Dataset` para los directorios de entrenamiento, validación y test, y devolver los tres datasets creados como\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "D-wm19eJhRJ-"
   },
   "outputs": [],
   "source": [
    "def create_datasets(traindir, valdir, testdir, image_size, batch_size):\n",
    "  \n",
    "  train_dataset = image_dataset_from_directory(\n",
    "      traindir,\n",
    "      image_size = (image_size, image_size),\n",
    "      batch_size = batch_size,\n",
    "      label_mode = 'categorical'\n",
    "  )\n",
    "  val_dataset = image_dataset_from_directory(\n",
    "      valdir,\n",
    "      image_size = (image_size, image_size),\n",
    "      batch_size = batch_size,\n",
    "      label_mode = 'categorical'\n",
    "  )\n",
    "  test_dataset = image_dataset_from_directory(\n",
    "      testdir,\n",
    "      image_size = (image_size, image_size),\n",
    "      batch_size = batch_size,\n",
    "      label_mode = 'categorical'\n",
    "  )\n",
    "  return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wGaULQsjwJz"
   },
   "source": [
    "Probemos que la función que has implementado funciona correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtZlNbHhiOIR",
    "outputId": "8fb28ba8-31ab-4d8a-944b-757806b28307"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6082 files belonging to 6 classes.\n",
      "Found 2108 files belonging to 6 classes.\n",
      "Found 2070 files belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size=32, batch_size=64)\n",
    "\n",
    "# Test whether all returned objects are valid Tensorflow datasets\n",
    "assert isinstance(train_dataset, tf.data.Dataset)\n",
    "assert isinstance(val_dataset, tf.data.Dataset)\n",
    "assert isinstance(test_dataset, tf.data.Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLIo6LBfhRJ-"
   },
   "source": [
    "¡Ahora que tenemos nuestros `Dataset` podemos entrenar una red profunda con ellos! Como ejemplo, vamos a construir una red convolucional extremadamente simple. Nótese cómo hemos añadido una capa especial de preprocesado llamada `Rescaling`, que será la encargada de normalizar los valores de los píxeles al rango [0, 1] cada vez que la red reciba una imagen.\n",
    "\n",
    "¡Ojo! Esta red tan simple no producirá errores al ejecutar, pero tiene algunos fallos de diseño que deberás corregir cuando crees tu propia red, más adelante en esta práctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "gbwmYJiLhRJ_"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, Convolution2D, Rescaling\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
    "model.add(Convolution2D(4, 3, activation='linear'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4v9OxA-AhRJ_"
   },
   "source": [
    "El método `fit` de un modelo Keras puede recibir un objeto `Dataset` como datos de entrenamiento, en lugar de un par de tensores (entradas, salidas). Como al construir los `Dataset` ya especificamos el tamaño de batch, no es necesario indicarlo ahora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ZXjs6CghRKA",
    "outputId": "57c034d2-c8aa-452d-9b9b-94c4c0bb6eca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 17s 75ms/step - loss: 1.7267 - accuracy: 0.2701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fba0beef640>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_ux64pchRKA"
   },
   "source": [
    "Análogamente, podemos evaluar el rendimiento de nuestro modelo sobre el `Dataset` de test de la siguiente manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_kQhkY-hRKA",
    "outputId": "588b4550-42df-45dd-9a0a-2c6fa11da500"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 4s 111ms/step - loss: 2.3198 - accuracy: 0.2647\n",
      "Loss 2.32, accuracy 26.5%\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_dataset)\n",
    "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlXf9OBehRKB"
   },
   "source": [
    "Este nivel de acierto puede parecer pobre, pero ten en cuenta que hemos usado un modelo muy simple y que el problema es de 6 clases. ¿Serás capaz de hacerlo mejor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kZnaNQIhRKB"
   },
   "source": [
    "## Construyendo tu propia red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNd8dC2whRKB"
   },
   "source": [
    "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
    "\n",
    "***\n",
    "\n",
    "<font color=#ad3e26>\n",
    "    Diseña una red neuronal profunda que obtenga el mejor acierto posible sobre los datos de test. Puedes usar los datos de entrenamiento y validación como te parezca mejor, pero <b>sólo</b> puedes usar los datos de test para evaluar el acierto de tu modelo. Debes obtener una red capaz de alcanzar al menos un 45% de acierto sobre los datos de test.\n",
    "</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9C9GNYihRKC"
   },
   "source": [
    "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
    "\n",
    "***\n",
    "<font color=#2655ad>\n",
    "    \n",
    "Algunas recomendaciones y estrategias que pueden ayudar a mejorar tu diseño de red:\n",
    "    \n",
    "- <b>Arquitectura</b>: usa todos los trucos que has aprendido en los ejercicios anteriores: capas convolucionales + pooling, activaciones ReLU, dropout... asegúrate también de utilizar un buen optimizador, con una función de error (loss) adecuada, así como una función de activación en la capa de salida que sea adecuada para esta clase de problema (clasificación multiclase).\n",
    "- <b>Desarrollo incremental</b>: empieza por redes pequeñas, con un número pequeño de parámetros, de forma que puedas comprobar rápidamente qué tal funcionan. Después, puedes hacer tu red más grande en tres direcciones: mayor tamaño de imágenes de entrada, más capas, y más kernels por capa convolucional o unidades por capa densa. Si aumentas el tamaño de las imágenes de entrada, asegúrate de añadir también más capas Convolution+Pooling, para que así a la capa Flatten solo lleguen imágenes muy pequeñas (10x10 píxeles o menos).\n",
    "- <b>Tamaño de imágenes</b>: configurar los `Dataset` para que carguen imágenes de mayor tamaño puede mejorar significativamente el rendimiento de tu red. Pero ten cuidado, ¡también puedes encontrarte errores de falta de memoria (CUDA memory error) si cargas imágenes a un tamaño demasiado grande! Para esta práctica, un tamaño mayor a 256 puede ser demasiado grande...\n",
    "- <b>Controlar el número de épocas</b>: Usa una <a href=\"https://keras.io/api/callbacks/early_stopping/\">**estrategia de EarlyStopping**</a> para monitorizar el loss de los datos de validación, y así detener el entrenamiento cuando tras un número de épocas esa loss no haya decrecido. Configurar la EarlyStopping para restaurar los mejores parámetros encontrados durante la optimización también puede resultarte útil.\n",
    "- <b>Sobreajuste</b>: si tu red obtiene un accuracy casi perfecto en entrenamiento, puede que estés sufriendo sobreajuste (aunque puede que no...). Prueba a incrementar el nivel de Dropout en las capas Dense para comprobar si así obtienes mejoras en el test.\n",
    "- <b>Demasiado bueno para ser verdad</b>: si tu red obtiene resultados muy buenos, del orden del 90% o más de acierto en test... sospecha. Es posible que estés mezclando los datos de entrenamiento y test.\n",
    "</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssDs0SGghRKC"
   },
   "source": [
    "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
    "\n",
    "***\n",
    "<font color=#259b4c>\n",
    "    \n",
    "Como ejercicio avanzado, añade las siguiente estrategias a tu red:\n",
    "\n",
    "- Usa **técnicas de \"image augmentation\"** para aumentar artificialmente tu dataset de entrenamiento. Para ello, explora las <a href=\"https://keras.io/api/layers/preprocessing_layers/image_augmentation/\">capas de augmentation disponibles en Keras</a>.\n",
    "- Usa capas de <a href=\"https://keras.io/api/layers/normalization_layers/batch_normalization/\">BatchNormalization</a> para facilitar el entrenamiento de la red. Revisa en las diapositivas de clase cuál es la forma adecuada de colocarlas en la red.\n",
    "    \n",
    "Usando estos trucos y los mencionados en el punto anterior, es posible obtener más de un 60% de acierto en el conjunto de test.\n",
    "\n",
    "</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Jgz7w3VqhRKC"
   },
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTACqj3-w_bh",
    "outputId": "ffd532b0-c5f6-4f5b-a7b6-9fb5edd49243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6082 files belonging to 6 classes.\n",
      "Found 2108 files belonging to 6 classes.\n",
      "Found 2070 files belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "Nombre = \"Red Sencilla\"\n",
    "Diseño = \"C(4) + Flatten + Dense(6)\"\n",
    "Estrategia_entrenamiento = \"Entrenamiento desde 0\"\n",
    "\n",
    "image_size = 32 #tamaño imagen de entrada\n",
    "channels = 3 #canales de color\n",
    "batch_size = 64 #tamaño del batch\n",
    "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size, batch_size) #llama a la funcion create_datasets\n",
    "\n",
    "mlp = Sequential()\n",
    "\n",
    "mlp.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3))) #capa de rescaling para normalizar valores\n",
    "mlp.add(Convolution2D (4, 3, activation = 'relu')) #Capa de convulcion 2D de 4 filtros con funcion de activacion RELU\n",
    "mlp.add(Flatten()) #Capa flatten para aplanar la entrada\n",
    "mlp.add(Dense(6, activation= 'softmax')) #Capa densa para 6 salidas y softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iOXPOajSXlZn",
    "outputId": "3814bc12-0a77-403d-c357-3765703acc10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_2 (Rescaling)     (None, 32, 32, 3)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 30, 30, 4)         112       \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 3600)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 21606     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,718\n",
      "Trainable params: 21,718\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "WDW3Znv8Xred"
   },
   "outputs": [],
   "source": [
    "mlp.compile (loss = 'categorical_crossentropy', optimizer = 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gh_h0ddEX1Wy",
    "outputId": "46a4fe24-0736-4d93-bdde-17fb3cc8ec2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "96/96 [==============================] - 10s 77ms/step - loss: 1.6642 - accuracy: 0.2841\n",
      "Epoch 2/5\n",
      "96/96 [==============================] - 9s 77ms/step - loss: 1.4698 - accuracy: 0.3982\n",
      "Epoch 3/5\n",
      "96/96 [==============================] - 9s 89ms/step - loss: 1.3998 - accuracy: 0.4395\n",
      "Epoch 4/5\n",
      "96/96 [==============================] - 9s 90ms/step - loss: 1.3615 - accuracy: 0.4530\n",
      "Epoch 5/5\n",
      "96/96 [==============================] - 8s 76ms/step - loss: 1.3235 - accuracy: 0.4740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fba0815d240>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(train_dataset, batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YOF6TaPfX4E6",
    "outputId": "f60e5b0e-6aea-4ecb-fc2a-004d52219d18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 3s 70ms/step - loss: 1.5058 - accuracy: 0.3947\n",
      "Test loss 1.5058262348175049\n",
      "Test accuracy 0.3946859836578369\n"
     ]
    }
   ],
   "source": [
    "score_mlp = mlp.evaluate(test_dataset)\n",
    "\n",
    "print(\"Test loss\", score_mlp[0]) #test loss\n",
    "print(\"Test accuracy\", score_mlp[1]) #Acuracy 0.394. Vamos a aumentar a 10 epochs para que tenga más oportunidades de aprender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9_8BJchYIZa",
    "outputId": "21d53e56-87db-442d-c6f7-f992b3965529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "96/96 [==============================] - 10s 95ms/step - loss: 1.2983 - accuracy: 0.4893\n",
      "Epoch 2/10\n",
      "96/96 [==============================] - 9s 89ms/step - loss: 1.2677 - accuracy: 0.5041\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - 8s 79ms/step - loss: 1.2404 - accuracy: 0.5164\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - 9s 83ms/step - loss: 1.2251 - accuracy: 0.5253\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - 9s 91ms/step - loss: 1.1973 - accuracy: 0.5388\n",
      "Epoch 6/10\n",
      "96/96 [==============================] - 9s 88ms/step - loss: 1.2099 - accuracy: 0.5358\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - 8s 74ms/step - loss: 1.1555 - accuracy: 0.5600\n",
      "Epoch 8/10\n",
      "96/96 [==============================] - 8s 74ms/step - loss: 1.1331 - accuracy: 0.5750\n",
      "Epoch 9/10\n",
      "96/96 [==============================] - 9s 88ms/step - loss: 1.1333 - accuracy: 0.5763\n",
      "Epoch 10/10\n",
      "96/96 [==============================] - 9s 84ms/step - loss: 1.0979 - accuracy: 0.5988\n",
      "33/33 [==============================] - 3s 70ms/step - loss: 1.5880 - accuracy: 0.3986\n",
      "Test loss 1.5879534482955933\n",
      "Test accuracy 0.39855071902275085\n"
     ]
    }
   ],
   "source": [
    "Nombre = \"Red Sencilla (10 epochs)\"\n",
    "\n",
    "mlp.fit(train_dataset, batch_size=64, epochs=10)\n",
    "\n",
    "score_mlp_10epochs = mlp.evaluate(test_dataset) #Evaluamos en test\n",
    "\n",
    "print(\"Test loss\", score_mlp_10epochs[0])\n",
    "print(\"Test accuracy\", score_mlp_10epochs[1]) #El acuracy es practicamente igual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5lFXkl_YYmg",
    "outputId": "231653d7-d20f-4234-d7bd-2702eb127f27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6082 files belonging to 6 classes.\n",
      "Found 2108 files belonging to 6 classes.\n",
      "Found 2070 files belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "Nombre = \"Red_1\"\n",
    "Diseño = \"C(32) + Pool(2) + Flatten + Dense(6)\"\n",
    "Estrategia_entrenamiento = \"Entrenamiento desde 0\"\n",
    "\n",
    "image_size = 32\n",
    "channels = 3\n",
    "batch_size = 64\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size, batch_size)\n",
    "\n",
    "convol = Sequential()\n",
    "\n",
    "convol.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3))) #Normalizar imagenes de entrada\n",
    "convol.add(Convolution2D( #Convulcion2D\n",
    "    32, \n",
    "    (kernel_size, kernel_size),\n",
    "    padding='valid', \n",
    "    input_shape=(image_size, image_size, 1),\n",
    "    activation=\"relu\"\n",
    ")) \n",
    "convol.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "convol.add(Flatten()) #Aplana en 1D\n",
    "convol.add(Dense(6, activation=\"softmax\")) #Capa densa de 6 salidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RGkHqZKZYvhi",
    "outputId": "aaa5e76e-3238-4fff-cadc-a781930dd519"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_3 (Rescaling)     (None, 32, 32, 3)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 7200)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 6)                 43206     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44,102\n",
      "Trainable params: 44,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "convol.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-iIFTLPhYylH",
    "outputId": "bca94581-819b-407f-e123-771c19c80c8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "96/96 - 9s - loss: 1.5901 - accuracy: 0.3428 - 9s/epoch - 91ms/step\n",
      "Epoch 2/20\n",
      "96/96 - 8s - loss: 1.4033 - accuracy: 0.4372 - 8s/epoch - 85ms/step\n",
      "Epoch 3/20\n",
      "96/96 - 9s - loss: 1.3394 - accuracy: 0.4719 - 9s/epoch - 92ms/step\n",
      "Epoch 4/20\n",
      "96/96 - 8s - loss: 1.2863 - accuracy: 0.4928 - 8s/epoch - 88ms/step\n",
      "Epoch 5/20\n",
      "96/96 - 8s - loss: 1.2463 - accuracy: 0.5181 - 8s/epoch - 78ms/step\n",
      "Epoch 6/20\n",
      "96/96 - 9s - loss: 1.2078 - accuracy: 0.5312 - 9s/epoch - 92ms/step\n",
      "Epoch 7/20\n",
      "96/96 - 10s - loss: 1.1735 - accuracy: 0.5567 - 10s/epoch - 100ms/step\n",
      "Epoch 8/20\n",
      "96/96 - 9s - loss: 1.1495 - accuracy: 0.5668 - 9s/epoch - 93ms/step\n",
      "Epoch 9/20\n",
      "96/96 - 8s - loss: 1.0969 - accuracy: 0.5891 - 8s/epoch - 82ms/step\n",
      "Epoch 10/20\n",
      "96/96 - 8s - loss: 1.0593 - accuracy: 0.6056 - 8s/epoch - 82ms/step\n",
      "Epoch 11/20\n",
      "96/96 - 9s - loss: 1.0296 - accuracy: 0.6207 - 9s/epoch - 94ms/step\n",
      "Epoch 12/20\n",
      "96/96 - 8s - loss: 0.9934 - accuracy: 0.6363 - 8s/epoch - 88ms/step\n",
      "Epoch 13/20\n",
      "96/96 - 9s - loss: 0.9647 - accuracy: 0.6501 - 9s/epoch - 95ms/step\n",
      "Epoch 14/20\n",
      "96/96 - 7s - loss: 0.9375 - accuracy: 0.6606 - 7s/epoch - 77ms/step\n",
      "Epoch 15/20\n",
      "96/96 - 9s - loss: 0.8971 - accuracy: 0.6789 - 9s/epoch - 89ms/step\n",
      "Epoch 16/20\n",
      "96/96 - 9s - loss: 0.8776 - accuracy: 0.6874 - 9s/epoch - 92ms/step\n",
      "Epoch 17/20\n",
      "96/96 - 10s - loss: 0.8433 - accuracy: 0.6996 - 10s/epoch - 99ms/step\n",
      "Epoch 18/20\n",
      "96/96 - 10s - loss: 0.8130 - accuracy: 0.7218 - 10s/epoch - 101ms/step\n",
      "Epoch 19/20\n",
      "96/96 - 9s - loss: 0.8146 - accuracy: 0.7180 - 9s/epoch - 93ms/step\n",
      "Epoch 20/20\n",
      "96/96 - 9s - loss: 0.7776 - accuracy: 0.7294 - 9s/epoch - 98ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb9f021b310>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convol.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "convol.fit(\n",
    "    train_dataset,\n",
    "    batch_size=64, \n",
    "    epochs=20, \n",
    "    verbose=2 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDkQ-v6wY1_9",
    "outputId": "eafa924d-1e4c-44ce-b6ea-4781f3a41c7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 3s 68ms/step - loss: 1.5785 - accuracy: 0.4295\n",
      "Test loss 1.5058262348175049\n",
      "Test accuracy 0.3946859836578369\n"
     ]
    }
   ],
   "source": [
    "score_convol = convol.evaluate(test_dataset)\n",
    "print(\"Test loss\", score_mlp[0])\n",
    "print(\"Test accuracy\", score_mlp[1]) #Acuracy del 0.39, implementaremos Dropout para evitar overfitting y detener el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OvhhCMluZB8n",
    "outputId": "e9cee087-e1eb-48c5-c76b-67e9fc986652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6082 files belonging to 6 classes.\n",
      "Found 2108 files belonging to 6 classes.\n",
      "Found 2070 files belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "Nombre = \"Red_2\"\n",
    "Diseño = \"C(32) + C(32) + Pool(2) + Flatten + Dense(128) + Dropout(0.5) + Dense(6)\"\n",
    "Estrategia_entrenamiento = \"Entrenamiento desde 0 y Dropout\"\n",
    "\n",
    "image_size = 32\n",
    "batch_size = 64\n",
    "kernel_size = 3\n",
    "pool_size = 2\n",
    "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size, batch_size)\n",
    "\n",
    "convol_dropout = Sequential()\n",
    "\n",
    "convol_dropout.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3))) #Normalizando pixeles\n",
    "convol_dropout.add(Convolution2D (32,  #Capa de convulcion 2D\n",
    "                              (kernel_size, kernel_size),\n",
    "                              padding='valid',\n",
    "                              input_shape=(image_size, image_size, 3), \n",
    "                              activation='relu'\n",
    "                              ))\n",
    "\n",
    "convol_dropout.add(Convolution2D (32, (kernel_size, kernel_size), activation='relu'))\n",
    "convol_dropout.add(MaxPooling2D(pool_size=(pool_size, pool_size))) #reduccion de dimensionalidad\n",
    "convol_dropout.add(Flatten()) #Aplanar las salidas a vector 1D\n",
    "convol_dropout.add(Dense(128, activation=\"relu\")) #Capa dense de 128 neuronas con activacion relu\n",
    "convol_dropout.add(Dropout(0.5)) #Dropout para prevenir el sobreajuste\n",
    "convol_dropout.add(Dense(6, activation=\"softmax\")) #Capa densa de salida de 6 neuronas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EPwiVLVQZE4e",
    "outputId": "0d8f8542-0136-4f5e-ea9d-e6af96719614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_4 (Rescaling)     (None, 32, 32, 3)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               802944    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 813,862\n",
      "Trainable params: 813,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "convol_dropout.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "pekGpD87ZG_j"
   },
   "outputs": [],
   "source": [
    "convol_dropout.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EoPN-a9MZJQw",
    "outputId": "66998566-62d3-4f8e-c1f4-33d5d1fff574"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "96/96 - 10s - loss: 1.6589 - accuracy: 0.3035 - 10s/epoch - 108ms/step\n",
      "Epoch 2/20\n",
      "96/96 - 9s - loss: 1.5170 - accuracy: 0.3594 - 9s/epoch - 91ms/step\n",
      "Epoch 3/20\n",
      "96/96 - 8s - loss: 1.4346 - accuracy: 0.4153 - 8s/epoch - 87ms/step\n",
      "Epoch 4/20\n",
      "96/96 - 8s - loss: 1.3683 - accuracy: 0.4410 - 8s/epoch - 78ms/step\n",
      "Epoch 5/20\n",
      "96/96 - 7s - loss: 1.3068 - accuracy: 0.4778 - 7s/epoch - 77ms/step\n",
      "Epoch 6/20\n",
      "96/96 - 9s - loss: 1.2512 - accuracy: 0.4964 - 9s/epoch - 89ms/step\n",
      "Epoch 7/20\n",
      "96/96 - 9s - loss: 1.1906 - accuracy: 0.5265 - 9s/epoch - 89ms/step\n",
      "Epoch 8/20\n",
      "96/96 - 7s - loss: 1.1380 - accuracy: 0.5515 - 7s/epoch - 77ms/step\n",
      "Epoch 9/20\n",
      "96/96 - 9s - loss: 1.0743 - accuracy: 0.5763 - 9s/epoch - 89ms/step\n",
      "Epoch 10/20\n",
      "96/96 - 9s - loss: 0.9863 - accuracy: 0.6118 - 9s/epoch - 89ms/step\n",
      "Epoch 11/20\n",
      "96/96 - 9s - loss: 0.9116 - accuracy: 0.6465 - 9s/epoch - 91ms/step\n",
      "Epoch 12/20\n",
      "96/96 - 7s - loss: 0.8651 - accuracy: 0.6620 - 7s/epoch - 76ms/step\n",
      "Epoch 13/20\n",
      "96/96 - 9s - loss: 0.7707 - accuracy: 0.7032 - 9s/epoch - 89ms/step\n",
      "Epoch 14/20\n",
      "96/96 - 9s - loss: 0.6972 - accuracy: 0.7302 - 9s/epoch - 89ms/step\n",
      "Epoch 15/20\n",
      "96/96 - 8s - loss: 0.6545 - accuracy: 0.7519 - 8s/epoch - 87ms/step\n",
      "Epoch 16/20\n",
      "96/96 - 8s - loss: 0.5856 - accuracy: 0.7789 - 8s/epoch - 80ms/step\n",
      "Epoch 17/20\n",
      "96/96 - 8s - loss: 0.5286 - accuracy: 0.8009 - 8s/epoch - 88ms/step\n",
      "Epoch 18/20\n",
      "96/96 - 9s - loss: 0.4911 - accuracy: 0.8109 - 9s/epoch - 89ms/step\n",
      "Epoch 19/20\n",
      "96/96 - 8s - loss: 0.4539 - accuracy: 0.8292 - 8s/epoch - 85ms/step\n",
      "Epoch 20/20\n",
      "96/96 - 8s - loss: 0.4565 - accuracy: 0.8256 - 8s/epoch - 81ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fba97dfb8b0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convol_dropout.fit(\n",
    "    train_dataset, \n",
    "    batch_size=64, \n",
    "    epochs=20, \n",
    "    verbose=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-WGTaHwZLhh",
    "outputId": "577b60a1-fa88-429f-e5e2-90ae61588953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 3s 69ms/step - loss: 1.9923 - accuracy: 0.4671\n",
      "Test loss 1.9922899007797241\n",
      "Test accuracy 0.4671497642993927\n"
     ]
    }
   ],
   "source": [
    "score_convol_dropout = convol_dropout.evaluate(test_dataset)\n",
    "print(\"Test loss\", score_convol_dropout[0])\n",
    "print(\"Test accuracy\", score_convol_dropout[1]) #acuray de 0.46. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nabqbaESZOst",
    "outputId": "a1edf6d7-2c88-441e-eaa8-029ad5a6389d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6082 files belonging to 6 classes.\n",
      "Found 2108 files belonging to 6 classes.\n",
      "Found 2070 files belonging to 6 classes.\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_5 (Rescaling)     (None, 32, 32, 3)         0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               802944    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 813,862\n",
      "Trainable params: 813,862\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.38425, saving model to best_model.h5\n",
      "96/96 - 13s - loss: 1.6260 - accuracy: 0.3055 - val_loss: 1.5123 - val_accuracy: 0.3843 - 13s/epoch - 134ms/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.38425 to 0.41366, saving model to best_model.h5\n",
      "96/96 - 11s - loss: 1.5053 - accuracy: 0.3808 - val_loss: 1.4383 - val_accuracy: 0.4137 - 11s/epoch - 118ms/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.41366\n",
      "96/96 - 10s - loss: 1.4571 - accuracy: 0.4027 - val_loss: 1.4316 - val_accuracy: 0.4089 - 10s/epoch - 105ms/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.41366 to 0.42410, saving model to best_model.h5\n",
      "96/96 - 11s - loss: 1.4090 - accuracy: 0.4283 - val_loss: 1.3851 - val_accuracy: 0.4241 - 11s/epoch - 112ms/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.42410 to 0.44687, saving model to best_model.h5\n",
      "96/96 - 14s - loss: 1.3598 - accuracy: 0.4480 - val_loss: 1.3581 - val_accuracy: 0.4469 - 14s/epoch - 144ms/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.44687 to 0.45731, saving model to best_model.h5\n",
      "96/96 - 11s - loss: 1.3375 - accuracy: 0.4651 - val_loss: 1.3919 - val_accuracy: 0.4573 - 11s/epoch - 118ms/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.45731\n",
      "96/96 - 11s - loss: 1.2673 - accuracy: 0.4921 - val_loss: 1.4061 - val_accuracy: 0.4464 - 11s/epoch - 116ms/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.45731 to 0.45778, saving model to best_model.h5\n",
      "96/96 - 14s - loss: 1.2292 - accuracy: 0.5176 - val_loss: 1.3438 - val_accuracy: 0.4578 - 14s/epoch - 141ms/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.45778 to 0.46015, saving model to best_model.h5\n",
      "96/96 - 14s - loss: 1.1680 - accuracy: 0.5398 - val_loss: 1.3768 - val_accuracy: 0.4602 - 14s/epoch - 143ms/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.46015 to 0.47296, saving model to best_model.h5\n",
      "96/96 - 11s - loss: 1.0990 - accuracy: 0.5694 - val_loss: 1.4083 - val_accuracy: 0.4730 - 11s/epoch - 116ms/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.47296\n",
      "96/96 - 11s - loss: 1.0553 - accuracy: 0.5880 - val_loss: 1.4203 - val_accuracy: 0.4658 - 11s/epoch - 116ms/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.47296\n",
      "96/96 - 11s - loss: 0.9907 - accuracy: 0.6185 - val_loss: 1.5026 - val_accuracy: 0.4578 - 11s/epoch - 115ms/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.47296\n",
      "96/96 - 11s - loss: 0.9417 - accuracy: 0.6338 - val_loss: 1.4378 - val_accuracy: 0.4620 - 11s/epoch - 116ms/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.47296\n",
      "96/96 - 10s - loss: 0.8634 - accuracy: 0.6698 - val_loss: 1.5428 - val_accuracy: 0.4507 - 10s/epoch - 109ms/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.47296\n",
      "96/96 - 11s - loss: 0.8030 - accuracy: 0.6976 - val_loss: 1.5376 - val_accuracy: 0.4644 - 11s/epoch - 111ms/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.47296\n",
      "96/96 - 11s - loss: 0.7348 - accuracy: 0.7134 - val_loss: 1.6871 - val_accuracy: 0.4511 - 11s/epoch - 116ms/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.47296\n",
      "96/96 - 11s - loss: 0.7002 - accuracy: 0.7269 - val_loss: 1.6931 - val_accuracy: 0.4521 - 11s/epoch - 116ms/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.47296\n",
      "96/96 - 14s - loss: 0.6211 - accuracy: 0.7642 - val_loss: 1.7302 - val_accuracy: 0.4492 - 14s/epoch - 142ms/step\n",
      "Epoch 18: early stopping\n",
      "33/33 [==============================] - 4s 90ms/step - loss: 1.4174 - accuracy: 0.4580\n",
      "Test loss 1.4174485206604004\n",
      "Test accuracy 0.4579710066318512\n"
     ]
    }
   ],
   "source": [
    "Nombre = \"Red_3\"\n",
    "Diseño = \"C(32) + C(32) + Pool(2) + Flatten + Dense(128) + Dropout(0.5) + Dense(6)\"\n",
    "Estrategia_entrenamiento = \"Entrenamiento desde 0, Dropout y EarlyStopping\"\n",
    "\n",
    "image_size = 32 #tamaño imagen\n",
    "batch_size = 64 #tamaño del lote\n",
    "kernel_size = 3 #tamaño kernel\n",
    "pool_size = 2 #tamaño pool\n",
    "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size, batch_size)\n",
    "\n",
    "convol3_dropout = Sequential()\n",
    "\n",
    "convol3_dropout.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
    "convol3_dropout.add(Convolution2D (32, \n",
    "                              (kernel_size, kernel_size),\n",
    "                              padding='valid',\n",
    "                              input_shape=(image_size, image_size, 3), \n",
    "                              activation='relu'\n",
    "                              ))\n",
    "\n",
    "convol3_dropout.add(Convolution2D (32, (kernel_size, kernel_size), activation='relu'))\n",
    "convol3_dropout.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "convol3_dropout.add(Flatten())\n",
    "convol3_dropout.add(Dense(128, activation=\"relu\"))\n",
    "convol3_dropout.add(Dropout(0.5))\n",
    "convol3_dropout.add(Dense(6, activation=\"softmax\"))\n",
    "\n",
    "convol3_dropout.summary()\n",
    "\n",
    "convol3_dropout.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "Es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10) #EarlyStopping para detener el entrenamiento\n",
    "Mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True) #Guarda el mejor modelo\n",
    "\n",
    "convol3_dropout.fit(train_dataset, validation_data = val_dataset, batch_size=64, epochs=50, verbose=2, callbacks=[Es, Mc])\n",
    "\n",
    "\n",
    "convol3_dropout_Es = load_model('best_model.h5')\n",
    "\n",
    "score_convol3_dropout = convol3_dropout_Es.evaluate(test_dataset)\n",
    "print(\"Test loss\", score_convol3_dropout[0])\n",
    "print(\"Test accuracy\", score_convol3_dropout[1]) #El accuray mejora, pero el modelo sigue sobreajustado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VkHuaXBZTlX",
    "outputId": "57aae026-e0bc-4bd5-fcb7-3fef084b2267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6082 files belonging to 6 classes.\n",
      "Found 2108 files belonging to 6 classes.\n",
      "Found 2070 files belonging to 6 classes.\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_6 (Rescaling)     (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 126, 126, 50)      1400      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 42, 42, 50)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 40, 40, 50)        22550     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 20, 20, 50)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 18, 18, 50)        22550     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 9, 9, 50)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 4050)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 128)               518528    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 599,594\n",
      "Trainable params: 599,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.36717, saving model to best_model.h5\n",
      "48/48 - 18s - loss: 1.6382 - accuracy: 0.2938 - val_loss: 1.5253 - val_accuracy: 0.3672 - 18s/epoch - 376ms/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.36717 to 0.39326, saving model to best_model.h5\n",
      "48/48 - 14s - loss: 1.4503 - accuracy: 0.4063 - val_loss: 1.4405 - val_accuracy: 0.3933 - 14s/epoch - 291ms/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.39326 to 0.42362, saving model to best_model.h5\n",
      "48/48 - 14s - loss: 1.3894 - accuracy: 0.4341 - val_loss: 1.4140 - val_accuracy: 0.4236 - 14s/epoch - 290ms/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.42362 to 0.44924, saving model to best_model.h5\n",
      "48/48 - 14s - loss: 1.3081 - accuracy: 0.4689 - val_loss: 1.3528 - val_accuracy: 0.4492 - 14s/epoch - 289ms/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.44924 to 0.50569, saving model to best_model.h5\n",
      "48/48 - 16s - loss: 1.2537 - accuracy: 0.5030 - val_loss: 1.2703 - val_accuracy: 0.5057 - 16s/epoch - 335ms/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.50569 to 0.50759, saving model to best_model.h5\n",
      "48/48 - 14s - loss: 1.2223 - accuracy: 0.5260 - val_loss: 1.2680 - val_accuracy: 0.5076 - 14s/epoch - 289ms/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.50759\n",
      "48/48 - 14s - loss: 1.1441 - accuracy: 0.5559 - val_loss: 1.3052 - val_accuracy: 0.5014 - 14s/epoch - 284ms/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.50759\n",
      "48/48 - 15s - loss: 1.0664 - accuracy: 0.5893 - val_loss: 1.3009 - val_accuracy: 0.4900 - 15s/epoch - 314ms/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.50759 to 0.52846, saving model to best_model.h5\n",
      "48/48 - 14s - loss: 1.0145 - accuracy: 0.6095 - val_loss: 1.2569 - val_accuracy: 0.5285 - 14s/epoch - 289ms/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.52846\n",
      "48/48 - 14s - loss: 0.9219 - accuracy: 0.6480 - val_loss: 1.2654 - val_accuracy: 0.5223 - 14s/epoch - 288ms/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: val_accuracy improved from 0.52846 to 0.54934, saving model to best_model.h5\n",
      "48/48 - 14s - loss: 0.8115 - accuracy: 0.6985 - val_loss: 1.2789 - val_accuracy: 0.5493 - 14s/epoch - 285ms/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.54934\n",
      "48/48 - 16s - loss: 0.7617 - accuracy: 0.7182 - val_loss: 1.4212 - val_accuracy: 0.5161 - 16s/epoch - 326ms/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.54934\n",
      "48/48 - 16s - loss: 0.6458 - accuracy: 0.7604 - val_loss: 1.5051 - val_accuracy: 0.5209 - 16s/epoch - 327ms/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.54934\n",
      "48/48 - 14s - loss: 0.5637 - accuracy: 0.7971 - val_loss: 1.5650 - val_accuracy: 0.5256 - 14s/epoch - 286ms/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.54934\n",
      "48/48 - 16s - loss: 0.4275 - accuracy: 0.8491 - val_loss: 1.7700 - val_accuracy: 0.5289 - 16s/epoch - 331ms/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.54934\n",
      "48/48 - 14s - loss: 0.3644 - accuracy: 0.8718 - val_loss: 1.9696 - val_accuracy: 0.4981 - 14s/epoch - 295ms/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.54934\n",
      "48/48 - 14s - loss: 0.2598 - accuracy: 0.9112 - val_loss: 2.1167 - val_accuracy: 0.5090 - 14s/epoch - 290ms/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.54934\n",
      "48/48 - 14s - loss: 0.2314 - accuracy: 0.9186 - val_loss: 2.2687 - val_accuracy: 0.5000 - 14s/epoch - 290ms/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.54934\n",
      "48/48 - 14s - loss: 0.2057 - accuracy: 0.9275 - val_loss: 2.5114 - val_accuracy: 0.4896 - 14s/epoch - 287ms/step\n",
      "Epoch 19: early stopping\n",
      "17/17 [==============================] - 4s 113ms/step - loss: 1.2834 - accuracy: 0.5488\n",
      "Test loss 1.2834340333938599\n",
      "Test accuracy 0.5487922430038452\n"
     ]
    }
   ],
   "source": [
    "Nombre = \"Red_4\"\n",
    "Diseño = \"C(50) + Pool(3) + C(50) + Pool(2) + C(50) + Pool(2) + Flatten +  Dense(128) + Dense(256) + Dense(6)\"\n",
    "Estrategia_entrenamiento = \"Entrenamiento desde 0 y EarlyStopping\"\n",
    "\n",
    "image_size = 128\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size, batch_size) # Al entrenar con imágenes más grandes, sobreescribimos los datos train,val y test con imágenes más grandes.\n",
    "\n",
    "kernel_size = 3\n",
    "pool_size = 3\n",
    "pool_size2 = 2\n",
    "pool_size3 = 2\n",
    "\n",
    "convol4 = Sequential()\n",
    "\n",
    "convol4.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
    "convol4.add(Convolution2D (50, \n",
    "                          (kernel_size, kernel_size),\n",
    "                          padding='valid',\n",
    "                          input_shape=(image_size, image_size, 3), \n",
    "                          activation='relu'))\n",
    "convol4.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "convol4.add(Convolution2D (50, (kernel_size, kernel_size), activation='relu'))\n",
    "convol4.add(MaxPooling2D(pool_size=(pool_size2, pool_size2)))\n",
    "convol4.add(Convolution2D (50, (kernel_size, kernel_size), activation='relu'))\n",
    "convol4.add(MaxPooling2D(pool_size=(pool_size3, pool_size3)))\n",
    "convol4.add(Flatten())\n",
    "convol4.add(Dense(128, activation=\"relu\"))\n",
    "convol4.add(Dense(256, activation=\"relu\"))\n",
    "convol4.add(Dense(6, activation=\"softmax\"))\n",
    "\n",
    "convol4.summary()\n",
    "\n",
    "convol4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "Es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "Mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "epochs = 50\n",
    "\n",
    "convol4.fit(train_dataset, validation_data= val_dataset, batch_size=batch_size, epochs=epochs, verbose=2, callbacks=[Es, Mc])\n",
    "\n",
    "convol4_Es = load_model('best_model.h5')\n",
    "\n",
    "score_convol4 = convol4_Es.evaluate(test_dataset)\n",
    "print(\"Test loss\", score_convol4[0])\n",
    "print(\"Test accuracy\", score_convol4[1]) #accuracy 0.54. El modelo mejora, pero sigue habiendo overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aagYac-nZeH9",
    "outputId": "6f9d5628-1322-4fed-e09b-bd8f75a28da4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6082 files belonging to 6 classes.\n",
      "Found 2108 files belonging to 6 classes.\n",
      "Found 2070 files belonging to 6 classes.\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " rescaling_7 (Rescaling)     (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 254, 254, 128)     3584      \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 84, 84, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 82, 82, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 27, 27, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 25, 25, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 8, 8, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               2097408   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 6)                 1542      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,397,702\n",
      "Trainable params: 2,397,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.32306, saving model to best_model.h5\n",
      "48/48 - 46s - loss: 1.6572 - accuracy: 0.2922 - val_loss: 1.5276 - val_accuracy: 0.3231 - 46s/epoch - 966ms/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.32306 to 0.44497, saving model to best_model.h5\n",
      "48/48 - 31s - loss: 1.4609 - accuracy: 0.4005 - val_loss: 1.3820 - val_accuracy: 0.4450 - 31s/epoch - 644ms/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.44497\n",
      "48/48 - 30s - loss: 1.3808 - accuracy: 0.4426 - val_loss: 1.3840 - val_accuracy: 0.4398 - 30s/epoch - 617ms/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.44497 to 0.47391, saving model to best_model.h5\n",
      "48/48 - 31s - loss: 1.3260 - accuracy: 0.4660 - val_loss: 1.3317 - val_accuracy: 0.4739 - 31s/epoch - 639ms/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.47391 to 0.48529, saving model to best_model.h5\n",
      "48/48 - 29s - loss: 1.2853 - accuracy: 0.4926 - val_loss: 1.2947 - val_accuracy: 0.4853 - 29s/epoch - 604ms/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: val_accuracy improved from 0.48529 to 0.51565, saving model to best_model.h5\n",
      "48/48 - 30s - loss: 1.2188 - accuracy: 0.5159 - val_loss: 1.2430 - val_accuracy: 0.5157 - 30s/epoch - 621ms/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.51565\n",
      "48/48 - 30s - loss: 1.1558 - accuracy: 0.5483 - val_loss: 1.4623 - val_accuracy: 0.4265 - 30s/epoch - 633ms/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.51565 to 0.51850, saving model to best_model.h5\n",
      "48/48 - 29s - loss: 1.1637 - accuracy: 0.5462 - val_loss: 1.2494 - val_accuracy: 0.5185 - 29s/epoch - 594ms/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: val_accuracy improved from 0.51850 to 0.55645, saving model to best_model.h5\n",
      "48/48 - 30s - loss: 1.0497 - accuracy: 0.5883 - val_loss: 1.2043 - val_accuracy: 0.5565 - 30s/epoch - 623ms/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.55645 to 0.55977, saving model to best_model.h5\n",
      "48/48 - 30s - loss: 0.9766 - accuracy: 0.6240 - val_loss: 1.2072 - val_accuracy: 0.5598 - 30s/epoch - 615ms/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.55977\n",
      "48/48 - 29s - loss: 0.8953 - accuracy: 0.6549 - val_loss: 1.2654 - val_accuracy: 0.5223 - 29s/epoch - 611ms/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.55977\n",
      "48/48 - 31s - loss: 0.8568 - accuracy: 0.6715 - val_loss: 1.2704 - val_accuracy: 0.5370 - 31s/epoch - 648ms/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.55977\n",
      "48/48 - 29s - loss: 0.7661 - accuracy: 0.7105 - val_loss: 1.2731 - val_accuracy: 0.5403 - 29s/epoch - 602ms/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.55977\n",
      "48/48 - 34s - loss: 0.7131 - accuracy: 0.7368 - val_loss: 1.3969 - val_accuracy: 0.5275 - 34s/epoch - 716ms/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.55977\n",
      "48/48 - 30s - loss: 0.6612 - accuracy: 0.7512 - val_loss: 1.4095 - val_accuracy: 0.5351 - 30s/epoch - 634ms/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.55977\n",
      "48/48 - 29s - loss: 0.5696 - accuracy: 0.7876 - val_loss: 1.5257 - val_accuracy: 0.5380 - 29s/epoch - 614ms/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.55977\n",
      "48/48 - 31s - loss: 0.4867 - accuracy: 0.8244 - val_loss: 1.6026 - val_accuracy: 0.5398 - 31s/epoch - 648ms/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.55977\n",
      "48/48 - 29s - loss: 0.4469 - accuracy: 0.8366 - val_loss: 1.7184 - val_accuracy: 0.5365 - 29s/epoch - 613ms/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.55977\n",
      "48/48 - 34s - loss: 0.3760 - accuracy: 0.8622 - val_loss: 1.8851 - val_accuracy: 0.5417 - 34s/epoch - 714ms/step\n",
      "Epoch 19: early stopping\n",
      "17/17 [==============================] - 7s 271ms/step - loss: 1.1768 - accuracy: 0.5512\n",
      "Test loss 1.1767522096633911\n",
      "Test accuracy 0.5512077212333679\n"
     ]
    }
   ],
   "source": [
    "Nombre = \"Red_5\"\n",
    "Diseño = \"C(128) + Pool(3) + C(128) + Pool(3) + C(128) + Pool(3) + Flatten + Dense(256) + Dropout(0.5) + Dense(6)\"\n",
    "Estrategia_entrenamiento = \"Entrenamiento desde 0, DropOut y EarlyStopping\"\n",
    "\n",
    "image_size = 256\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size, batch_size)\n",
    "\n",
    "kernel_size = 3\n",
    "pool_size = 3\n",
    "\n",
    "\n",
    "convol5 = Sequential()\n",
    "\n",
    "convol5.add(Rescaling(scale=1./255, input_shape=(image_size, image_size, 3)))\n",
    "convol5.add(Convolution2D (128, \n",
    "                          (kernel_size, kernel_size),\n",
    "                          padding='valid',\n",
    "                          input_shape=(image_size, image_size, 3), \n",
    "                          activation='relu'))\n",
    "convol5.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "convol5.add(Convolution2D (128, (kernel_size, kernel_size), activation='relu'))\n",
    "convol5.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "convol5.add(Convolution2D (128, (kernel_size, kernel_size), activation='relu'))\n",
    "convol5.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "convol5.add(Flatten())\n",
    "convol5.add(Dense(256, activation=\"relu\"))\n",
    "convol5.add(Dropout(0.5))\n",
    "convol5.add(Dense(6, activation=\"softmax\"))\n",
    "\n",
    "convol5.summary()\n",
    "\n",
    "convol5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "Es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "Mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "epochs = 50\n",
    "\n",
    "convol5.fit(train_dataset, validation_data= val_dataset, batch_size=batch_size, epochs=epochs, verbose=2, callbacks=[Es, Mc])\n",
    "\n",
    "convol5_Es = load_model('best_model.h5')\n",
    "\n",
    "score_convol5 = convol5_Es.evaluate(test_dataset)\n",
    "print(\"Test loss\", score_convol5[0])\n",
    "print(\"Test accuracy\", score_convol5[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrLe9mFwhRKC"
   },
   "source": [
    "## Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a5xs6_RhRKC"
   },
   "source": [
    "Aunque diseñar nuestra propia red puede producir algunos resultados aceptables, suele ser mejor aprovechar el conocimiento ya existente en una red pre-entrenada. Esto no solo nos lleva a resultados mejores, sino que además nos ahorra mucho tiempo de diseño de la red. Para ello, el módulo [Keras Applications](https://keras.io/api/applications/) contiene varios diseños de redes listos para su uso. Por ejemplo, para hacer uso de la famosa red VGG16 hacemos lo siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "fTLIqMTAhRKD"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "vgg16_model = VGG16(include_top=False, input_shape=(image_size, image_size, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZUvx4zdhRKD"
   },
   "source": [
    "Por defecto, todas las redes de Keras Applications están precargadas con los pesos que se obtuvieron al entrenar la red sobre el dataset de la competición de [ImageNet](http://www.image-net.org/). Para adaptar la red a nuestro problema, hemos necesitado especificar la resolución de nuestras imágenes (`input_shape`), así como eliminar las capas de salida (`top`) de la red original, dado que nosotros tendremos un número diferente de clases.\n",
    "\n",
    "Ahora, ¿cómo hacemos para transferir el aprendizaje de esta red? Vamos a ver cómo implementar la estrategia de \"bottleneck features\". En primer lugar, marcamos el modelo VGG16 como no entrenable, para que sus parámetros se mantengan congelados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "iWsuQg0phRKD"
   },
   "outputs": [],
   "source": [
    "vgg16_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljtS1PhRhRKE"
   },
   "source": [
    "Hecho esto, vamos a construir una red neuronal que incluya la VGG16 como una de sus \"capas\". Es necesario tener en cuenta que la red VGG16 se entrenó realizando una normalización muy específica de las imágenes de entrenamiento, y nosotros debemos seguir ese mismo proceso para que la red se comporte correctamente. Convenientemente, Keras también nos da una funcionalidad para replicar la normalización que la VGG16 necesita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "ZjFpoNRIhRKE"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTBk5uRChRKF"
   },
   "source": [
    "Podemos probar esta normalización con alguna de las imágenes de nuestro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6ukW26shRKF",
    "outputId": "1c7f95d3-448b-4030-9878-4adca970c60e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalizing: [[[170.   188.   188.  ]\n",
      "  [171.   189.   189.  ]\n",
      "  [171.   189.   189.  ]]\n",
      "\n",
      " [[170.   188.   188.  ]\n",
      "  [171.   189.   189.  ]\n",
      "  [171.75 189.75 189.75]]\n",
      "\n",
      " [[170.   188.   188.  ]\n",
      "  [171.5  189.5  189.5 ]\n",
      "  [172.   190.   190.  ]]]\n",
      "After normalizing: [[[84.061 71.221 46.32 ]\n",
      "  [85.061 72.221 47.32 ]\n",
      "  [85.061 72.221 47.32 ]]\n",
      "\n",
      " [[84.061 71.221 46.32 ]\n",
      "  [85.061 72.221 47.32 ]\n",
      "  [85.811 72.971 48.07 ]]\n",
      "\n",
      " [[84.061 71.221 46.32 ]\n",
      "  [85.561 72.721 47.82 ]\n",
      "  [86.061 73.221 48.32 ]]]\n"
     ]
    }
   ],
   "source": [
    "for X_batch, _ in train_dataset:\n",
    "    break\n",
    "    \n",
    "print(f\"Before normalizing: {X_batch[0, :3, :3, :]}\")\n",
    "print(f\"After normalizing: {preprocess_input(X_batch)[0, :3, :3, :]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sx3EQeEThRKF"
   },
   "source": [
    "La normalización realizada por la VGG16 consiste en invertir el orden de los canales de color (RGB -> BGR), y restar los valores medios sobre todo el dataset ImageNet para cada canal de color por separado. Afortunadamente, la función `preprocess_input` que hemos importado hace todo este trabajo por nosotros. Además, podemos incrustar esta función como la primera capa de nuestra red, cumpliendo el papel de la capa `Rescaling` que utilizamos en el apartado anterior. Esto es posible gracias a la capa `Lambda`, que permite construir una capa Keras en base a cualquier función de Tensorflow. De modo que, vamos a comenzar nuestro diseño de red con esta capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "DnSoUUrQhRKG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Lambda(preprocess_input, input_shape=(image_size, image_size, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k83-_MmdhRKG"
   },
   "source": [
    "Tras esto, podemos añadir toda la red VGG16 como si fuera una nueva capa, y nuestras propias capas después de ella. A continuación tenemos un ejemplo de esta clase de diseño, aunque es importante destacar que es un diseño muy sencillo que contiene algunos fallos; una red real para hacer transfer learning debería tener un diseño mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9f0RdEumhRKG",
    "outputId": "29b2e145-90c5-4bd1-8316-d27d9508ee0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda_1 (Lambda)           (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 8, 8, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 32768)             0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 6)                 196614    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,911,302\n",
      "Trainable params: 196,614\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(vgg16_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NueAGF7DhRKG"
   },
   "source": [
    "Observa cómo en el resumen del modelo podemos ver que la red completa tiene millones de parámetros, pero dado que hemos congelado toda la parte de la red perteneciente a la VGG16, solo unos pocos miles de parámetros son entrenables (trainable): aquellos correspondientes a la capa Dense que hemos colocado al final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn7iMKlQhRKH"
   },
   "source": [
    "Ya podemos compilar y entrenar el modelo a la manera habitual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5eBG6zKwhRKH",
    "outputId": "2af8352f-7092-44f6-cb49-16c3fa2deed7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 70s 1s/step - loss: 377.9374 - accuracy: 0.5878\n",
      "17/17 [==============================] - 19s 978ms/step - loss: 79.4171 - accuracy: 0.7599\n",
      "Loss 79.4, accuracy 76.0%\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=[\"accuracy\"])\n",
    "model.fit(train_dataset, epochs=1)\n",
    "\n",
    "loss, acc = model.evaluate(test_dataset)\n",
    "print(f\"Loss {loss:.3}, accuracy {acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Sb7SVSkhRKH"
   },
   "source": [
    "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
    "\n",
    "***\n",
    "\n",
    "<font color=#ad3e26>\n",
    "    Usando la estrategia \"bottleneck\" presentada, implementa una red que haga transfer learning desde la red VGG16, con un diseño correcto. Si lo haces adecuadamente, esta red debe obtener mejores resultados que con la red que diseñaste en el apartado anterior, y con al menos un 80% de acierto sobre el conjunto de test.\n",
    "</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RzJR4LvghLz4",
    "outputId": "b3149e7d-506a-4f3e-bd0f-9a605a5d8d60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6082 files belonging to 6 classes.\n",
      "Found 2108 files belonging to 6 classes.\n",
      "Found 2070 files belonging to 6 classes.\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lambda_12 (Lambda)          (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
      "                                                                 \n",
      " flatten_20 (Flatten)        (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 128)               1048704   \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,797,190\n",
      "Trainable params: 1,082,502\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.23624, saving model to best_model.h5\n",
      "47/47 - 20s - loss: 4.4259 - accuracy: 0.2223 - val_loss: 1.7684 - val_accuracy: 0.2362 - 20s/epoch - 416ms/step\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 2375.78125 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.23624\n",
      "47/47 - 5s - loss: 0.0000e+00 - accuracy: 0.0000e+00 - val_loss: 1.7684 - val_accuracy: 0.2362 - 5s/epoch - 104ms/step\n",
      "17/17 [==============================] - 6s 219ms/step - loss: 1.7756 - accuracy: 0.2401\n",
      "Test loss 1.7755861282348633\n",
      "Test accuracy 0.24009661376476288\n"
     ]
    }
   ],
   "source": [
    "Nombre = \"VGG16_1\"\n",
    "Diseño = \"VGG16  + Flatten + Dense(128) + DropOut(0.5) + Dense(128) + DropOut(0.5) + Dense(128) + DropOut(0.5) + Dense(6)\"\n",
    "Estrategia_entrenamiento = \"Bottleneck, Aumento, DropOut y EarlyStopping\"\n",
    "\n",
    "image_size = 128\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size, batch_size)\n",
    "\n",
    "channels = 3\n",
    "kernel_size = 3 \n",
    "\n",
    "\n",
    "VGG16_1 = Sequential()\n",
    "VGG16_1.add(Lambda(preprocess_input, input_shape=(image_size, image_size, 3)))\n",
    "\n",
    "VGG16_1.add(vgg16_model)\n",
    "VGG16_1.add(Flatten())\n",
    "VGG16_1.add(Dense(128, activation=\"relu\"))\n",
    "VGG16_1.add(Dropout(0.5))\n",
    "VGG16_1.add(Dense(128, activation=\"relu\"))\n",
    "VGG16_1.add(Dropout(0.5))\n",
    "VGG16_1.add(Dense(128, activation=\"relu\"))\n",
    "VGG16_1.add(Dropout(0.5))\n",
    "VGG16_1.add(Dense(6, activation= 'softmax'))\n",
    "\n",
    "VGG16_1.summary()\n",
    "\n",
    "VGG16_1.compile(loss='categorical_crossentropy', optimizer= 'adam', metrics=[\"accuracy\"])\n",
    "\n",
    "Es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "Mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "epochs = 50\n",
    "\n",
    "VGG16_1.fit(train_dataset, validation_data=(val_dataset), batch_size=batch_size, steps_per_epoch=6082/batch_size, epochs = epochs, verbose=2, callbacks=[Es, Mc])\n",
    "\n",
    "VGG16_1_Es = load_model('best_model.h5')\n",
    "\n",
    "score_VGG16_1 = VGG16_1_Es.evaluate(test_dataset)\n",
    "print(\"Test loss\", score_VGG16_1[0])\n",
    "print(\"Test accuracy\", score_VGG16_1[1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pBz63y-4j6KL",
    "outputId": "af1f3a2a-fea0-4e5c-af01-1de38c3c9a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6082 files belonging to 6 classes.\n",
      "Found 2108 files belonging to 6 classes.\n",
      "Found 2070 files belonging to 6 classes.\n",
      "Epoch 1/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 2.7278 - accuracy: 0.3357\n",
      "Epoch 1: val_accuracy improved from -inf to 0.59535, saving model to best_model.h5\n",
      "48/48 [==============================] - 70s 1s/step - loss: 2.7278 - accuracy: 0.3357 - val_loss: 1.1278 - val_accuracy: 0.5954\n",
      "Epoch 2/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.9986 - accuracy: 0.6493\n",
      "Epoch 2: val_accuracy improved from 0.59535 to 0.74288, saving model to best_model.h5\n",
      "48/48 [==============================] - 57s 1s/step - loss: 0.9986 - accuracy: 0.6493 - val_loss: 0.7328 - val_accuracy: 0.7429\n",
      "Epoch 3/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.6872 - accuracy: 0.7738\n",
      "Epoch 3: val_accuracy improved from 0.74288 to 0.78083, saving model to best_model.h5\n",
      "48/48 [==============================] - 57s 1s/step - loss: 0.6872 - accuracy: 0.7738 - val_loss: 0.6806 - val_accuracy: 0.7808\n",
      "Epoch 4/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.4481 - accuracy: 0.8504\n",
      "Epoch 4: val_accuracy improved from 0.78083 to 0.81120, saving model to best_model.h5\n",
      "48/48 [==============================] - 57s 1s/step - loss: 0.4481 - accuracy: 0.8504 - val_loss: 0.6494 - val_accuracy: 0.8112\n",
      "Epoch 5/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.3068 - accuracy: 0.9005\n",
      "Epoch 5: val_accuracy improved from 0.81120 to 0.82638, saving model to best_model.h5\n",
      "48/48 [==============================] - 58s 1s/step - loss: 0.3068 - accuracy: 0.9005 - val_loss: 0.6557 - val_accuracy: 0.8264\n",
      "Epoch 6/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.2358 - accuracy: 0.9262\n",
      "Epoch 6: val_accuracy did not improve from 0.82638\n",
      "48/48 [==============================] - 57s 1s/step - loss: 0.2358 - accuracy: 0.9262 - val_loss: 0.7702 - val_accuracy: 0.8012\n",
      "Epoch 7/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.9497\n",
      "Epoch 7: val_accuracy did not improve from 0.82638\n",
      "48/48 [==============================] - 56s 1s/step - loss: 0.1770 - accuracy: 0.9497 - val_loss: 0.7482 - val_accuracy: 0.8126\n",
      "Epoch 8/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.1105 - accuracy: 0.9670\n",
      "Epoch 8: val_accuracy did not improve from 0.82638\n",
      "48/48 [==============================] - 57s 1s/step - loss: 0.1105 - accuracy: 0.9670 - val_loss: 0.8979 - val_accuracy: 0.8169\n",
      "Epoch 9/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.0865 - accuracy: 0.9758\n",
      "Epoch 9: val_accuracy did not improve from 0.82638\n",
      "48/48 [==============================] - 57s 1s/step - loss: 0.0865 - accuracy: 0.9758 - val_loss: 0.9161 - val_accuracy: 0.8197\n",
      "Epoch 10/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.1107 - accuracy: 0.9691\n",
      "Epoch 10: val_accuracy did not improve from 0.82638\n",
      "48/48 [==============================] - 57s 1s/step - loss: 0.1107 - accuracy: 0.9691 - val_loss: 0.9926 - val_accuracy: 0.8031\n",
      "Epoch 11/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.0960 - accuracy: 0.9752\n",
      "Epoch 11: val_accuracy did not improve from 0.82638\n",
      "48/48 [==============================] - 57s 1s/step - loss: 0.0960 - accuracy: 0.9752 - val_loss: 0.9643 - val_accuracy: 0.8131\n",
      "Epoch 12/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9811\n",
      "Epoch 12: val_accuracy did not improve from 0.82638\n",
      "48/48 [==============================] - 61s 1s/step - loss: 0.0774 - accuracy: 0.9811 - val_loss: 0.9968 - val_accuracy: 0.8107\n",
      "Epoch 13/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9877\n",
      "Epoch 13: val_accuracy did not improve from 0.82638\n",
      "48/48 [==============================] - 57s 1s/step - loss: 0.0392 - accuracy: 0.9877 - val_loss: 1.1066 - val_accuracy: 0.8159\n",
      "Epoch 14/50\n",
      "48/48 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9857\n",
      "Epoch 14: val_accuracy did not improve from 0.82638\n",
      "48/48 [==============================] - 57s 1s/step - loss: 0.0494 - accuracy: 0.9857 - val_loss: 1.1620 - val_accuracy: 0.8254\n",
      "Epoch 14: early stopping\n",
      "17/17 [==============================] - 16s 774ms/step - loss: 0.5981 - accuracy: 0.8285\n",
      "Test loss 0.5980625748634338\n",
      "Test accuracy 0.8285024166107178\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "Nombre = \"VGG16_2\"\n",
    "Diseño = \"VGG16  + C(128) + Pool(3) + C(128) + Pool(3) + GAP2D + Dense(512) + DropOut(0.5) + Dense(512) + DropOut(0.5) + Dense(512) + DropOut(0.5) + Dense(6)\"\n",
    "Estrategia_entrenamiento = \"Bottleneck, Aumento, DropOut, EarlyStopping y GlobalAveragePooling2D\"\n",
    "\n",
    "image_size = 256\n",
    "batch_size = 128\n",
    "train_dataset, val_dataset, test_dataset = create_datasets(TRAINDIR, VALDIR, TESTDIR, image_size, batch_size)\n",
    "\n",
    "vgg16_model = VGG16(include_top=False, input_shape=(image_size, image_size, 3))\n",
    "\n",
    "vgg16_model.trainable = False\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "\n",
    "\n",
    "channels = 3\n",
    "pool_size = 3 \n",
    "kernel_size = 3 \n",
    "\n",
    "\n",
    "VGG16_2 = Sequential()\n",
    "\n",
    "VGG16_2.add(Lambda(preprocess_input, input_shape=(image_size, image_size, 3)))\n",
    "\n",
    "VGG16_2.add(vgg16_model)\n",
    "\n",
    "VGG16_2.add(Convolution2D(128,\n",
    "                           (kernel_size, kernel_size),\n",
    "                           padding='same',\n",
    "                           activation='relu'))\n",
    "VGG16_2.add(MaxPooling2D(pool_size=pool_size, strides=4))\n",
    "VGG16_2.add(GlobalAveragePooling2D(data_format=None, keepdims=False))\n",
    "\n",
    "VGG16_2.add(Dense(512, activation='relu'))\n",
    "VGG16_2.add(Dropout(0.5))\n",
    "\n",
    "VGG16_2.add(Dense(512, activation='relu'))\n",
    "VGG16_2.add(Dropout(0.5))\n",
    "\n",
    "VGG16_2.add(Dense(512, activation='relu'))\n",
    "VGG16_2.add(Dropout(0.5))\n",
    "\n",
    "VGG16_2.add(Dense(6, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "VGG16_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "Es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "Mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "epochs = 50\n",
    "\n",
    "VGG16_2.fit(train_dataset, validation_data=val_dataset, epochs=epochs, callbacks=[Es, Mc])\n",
    "\n",
    "VGG16_2_Es = load_model('best_model.h5')\n",
    "\n",
    "score_VGG16_2 = VGG16_2_Es.evaluate(test_dataset)\n",
    "print(\"Test loss\", score_VGG16_2[0])\n",
    "print(\"Test accuracy\", score_VGG16_2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rm2LQa-mhRKI"
   },
   "source": [
    "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
    "\n",
    "***\n",
    "<font color=#2655ad>\n",
    "    \n",
    "Algunos consejos para mejorar tu diseño de red:\n",
    "    \n",
    "- Incluye una o más capas Dense, con sus funciones de activación apropiadas, antes de la capa de salida.\n",
    "- Intenta usar una capa de tipo [GlobalAveragePooling](https://keras.io/api/layers/pooling_layers/global_average_pooling2d/) en lugar de la capa Flatten. Esta capa calcula una media de todos los valores de píxeles para cada canal, y en algunas ocasiones produce mejores resultados que la capa Flatten.\n",
    "- ¡Y no olvides todos los consejos del apartado anterior! También aplican aquí.\n",
    "</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sl6bbHEWhRKI"
   },
   "source": [
    "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
    "\n",
    "***\n",
    "<font color=#259b4c>\n",
    "\n",
    "Para mejorar aún más los resultados de tu red, utiliza las siguientes ideas:\n",
    "\n",
    "- Usa las estrategias PRO del ejercicio anterio.\n",
    "- Prueba otras redes pre-entrenadas de <a href=\"https://keras.io/api/applications/\">Keras Applications</a>, como ResNet, Xception o EfficientNet.\n",
    "- Usa una estrategia de transfer learning más avanzada, como fine-tuning o una combinación de bottleneck features y fine-tuning. Revisa las diapositivas de clase para saber cómo.\n",
    "   \n",
    "Si empleas todos estos trucos, es posible alcanzar más de un 90% de acierto en el conjunto de test.\n",
    "\n",
    "</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qLJ8IgANhRKI",
    "outputId": "977ebf37-4889-4bd7-f142-508933b69940"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Estrategia</th>\n",
       "      <th>Diseño</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Red Sencilla</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.39</td>\n",
       "      <td>Entrenamiento desde 0</td>\n",
       "      <td>C(4) + Flatten + Dense(6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Red Sencilla(10)</td>\n",
       "      <td>1.58</td>\n",
       "      <td>0.39</td>\n",
       "      <td>Entrenamiento desde 0</td>\n",
       "      <td>C(4) + Flatten + Dense(6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Red 1</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.39</td>\n",
       "      <td>Entrenamiento desde 0</td>\n",
       "      <td>C(32) + Pool(2) + Flatten + Dense(6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Red2</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>Entrenamiento desde 0 y Dropout</td>\n",
       "      <td>C(32) + C(32) + Pool(2) + Flatten + Dense(128) + Dropout(0.5) + Dense(6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Red3</td>\n",
       "      <td>1.41</td>\n",
       "      <td>0.45</td>\n",
       "      <td>Entrenamiento desde 0, Dropout y EarlyStopping</td>\n",
       "      <td>C(32) + C(32) + Pool(2) + Flatten + Dense(128) + Dropout(0.5) + Dense(6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Red 4</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.54</td>\n",
       "      <td>Entrenamiento desde 0, Dropout y EarlyStopping</td>\n",
       "      <td>C(50) + Pool(3) + C(50) + Pool(2) + C(50) + Pool(2) + Flatten +  Dense(128) + Dense(256) + Dense(6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Red5</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.55</td>\n",
       "      <td>Entrenamiento desde 0, Dropout y EarlyStopping</td>\n",
       "      <td>C(128) + Pool(3) + C(128) + Pool(3) + C(128) + Pool(3) + Flatten + Dense(256) + Dropout(0.5) + Dense(6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>VGG16_1</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.24</td>\n",
       "      <td>Bottleneck, Aumento, DropOut y EarlyStopping</td>\n",
       "      <td>VGG16  + Flatten + Dense(128) + DropOut(0.5) + Dense(128) + DropOut(0.5) + Dense(128) + DropOut(0.5) + Dense(6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VGG16_2</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.82</td>\n",
       "      <td>Bottleneck, Aumento, DropOut, EarlyStopping y GlobalAveragePooling2D</td>\n",
       "      <td>VGG16  + C(128) + Pool(3) + C(128) + Pool(3) + GAP2D + Dense(512) + DropOut(0.5) + Dense(512) + DropOut(0.5) + Dense(512) + DropOut(0.5) + Dense(6)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model Name  Test Loss  Test Accuracy  \\\n",
       "0      Red Sencilla       1.50           0.39   \n",
       "1  Red Sencilla(10)       1.58           0.39   \n",
       "2             Red 1       1.50           0.39   \n",
       "3              Red2       1.99           0.46   \n",
       "4              Red3       1.41           0.45   \n",
       "5             Red 4       1.28           0.54   \n",
       "6              Red5       1.17           0.55   \n",
       "7           VGG16_1       1.77           0.24   \n",
       "8           VGG16_2       0.59           0.82   \n",
       "\n",
       "                                                             Estrategia  \\\n",
       "0                                                 Entrenamiento desde 0   \n",
       "1                                                 Entrenamiento desde 0   \n",
       "2                                                 Entrenamiento desde 0   \n",
       "3                                       Entrenamiento desde 0 y Dropout   \n",
       "4                        Entrenamiento desde 0, Dropout y EarlyStopping   \n",
       "5                        Entrenamiento desde 0, Dropout y EarlyStopping   \n",
       "6                        Entrenamiento desde 0, Dropout y EarlyStopping   \n",
       "7                          Bottleneck, Aumento, DropOut y EarlyStopping   \n",
       "8  Bottleneck, Aumento, DropOut, EarlyStopping y GlobalAveragePooling2D   \n",
       "\n",
       "                                                                                                                                                Diseño  \n",
       "0                                                                                                                            C(4) + Flatten + Dense(6)  \n",
       "1                                                                                                                            C(4) + Flatten + Dense(6)  \n",
       "2                                                                                                                 C(32) + Pool(2) + Flatten + Dense(6)  \n",
       "3                                                                             C(32) + C(32) + Pool(2) + Flatten + Dense(128) + Dropout(0.5) + Dense(6)  \n",
       "4                                                                             C(32) + C(32) + Pool(2) + Flatten + Dense(128) + Dropout(0.5) + Dense(6)  \n",
       "5                                                  C(50) + Pool(3) + C(50) + Pool(2) + C(50) + Pool(2) + Flatten +  Dense(128) + Dense(256) + Dense(6)  \n",
       "6                                              C(128) + Pool(3) + C(128) + Pool(3) + C(128) + Pool(3) + Flatten + Dense(256) + Dropout(0.5) + Dense(6)  \n",
       "7                                      VGG16  + Flatten + Dense(128) + DropOut(0.5) + Dense(128) + DropOut(0.5) + Dense(128) + DropOut(0.5) + Dense(6)  \n",
       "8  VGG16  + C(128) + Pool(3) + C(128) + Pool(3) + GAP2D + Dense(512) + DropOut(0.5) + Dense(512) + DropOut(0.5) + Dense(512) + DropOut(0.5) + Dense(6)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_names = [\"Red Sencilla\", \"Red Sencilla(10)\", \"Red 1\", \"Red2\", \"Red3\",\"Red 4\",\"Red5\",\"VGG16_1\",\"VGG16_2\"]\n",
    "test_loss = [1.50, 1.58, 1.50,1.99,1.41,1.28,1.17,1.77,0.59]\n",
    "test_accuracy = [0.39, 0.39, 0.39, 0.46, 0.45,0.54,0.55,0.24,0.82]\n",
    "Estrategia = [\"Entrenamiento desde 0\", \"Entrenamiento desde 0\", \"Entrenamiento desde 0\", \"Entrenamiento desde 0 y Dropout\",\"Entrenamiento desde 0, Dropout y EarlyStopping\",\"Entrenamiento desde 0, Dropout y EarlyStopping\",\"Entrenamiento desde 0, Dropout y EarlyStopping\",\"Bottleneck, Aumento, DropOut y EarlyStopping\",\"Bottleneck, Aumento, DropOut, EarlyStopping y GlobalAveragePooling2D\"]\n",
    "Diseño = [\"C(4) + Flatten + Dense(6)\",\"C(4) + Flatten + Dense(6)\",\"C(32) + Pool(2) + Flatten + Dense(6)\",\"C(32) + C(32) + Pool(2) + Flatten + Dense(128) + Dropout(0.5) + Dense(6)\",\"C(32) + C(32) + Pool(2) + Flatten + Dense(128) + Dropout(0.5) + Dense(6)\",\"C(50) + Pool(3) + C(50) + Pool(2) + C(50) + Pool(2) + Flatten +  Dense(128) + Dense(256) + Dense(6)\",\"C(128) + Pool(3) + C(128) + Pool(3) + C(128) + Pool(3) + Flatten + Dense(256) + Dropout(0.5) + Dense(6)\",\"VGG16  + Flatten + Dense(128) + DropOut(0.5) + Dense(128) + DropOut(0.5) + Dense(128) + DropOut(0.5) + Dense(6)\",\"VGG16  + C(128) + Pool(3) + C(128) + Pool(3) + GAP2D + Dense(512) + DropOut(0.5) + Dense(512) + DropOut(0.5) + Dense(512) + DropOut(0.5) + Dense(6)\"]\n",
    "\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame({\n",
    "    'Model Name': model_names,\n",
    "    'Test Loss': test_loss,\n",
    "    'Test Accuracy': test_accuracy,\n",
    "    'Estrategia': Estrategia,\n",
    "    'Diseño': Diseño\n",
    "})\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVJc7DMJhRKK"
   },
   "source": [
    "## Informe final y resumen de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PV2c2JCKhRKK"
   },
   "source": [
    "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
    "\n",
    "***\n",
    "\n",
    "<font color=#ad3e26>\n",
    "Escribe en la siguiente celda un pequeño informe con:\n",
    "    <ul>\n",
    "        <li>Una tabla de resultados, indicando qué diseños de red has probado y qué resultados en test has obtenido. Puede usar un estilo de tabla como el que se muestra abajo.</li>\n",
    "        <li>De las estrategias y diseños que has ido probando, ¿qué ha funcionado y qué no?</li>\n",
    "        <li>¿Qué has aprendido con esta práctica?\n",
    "    </ul>\n",
    "</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#la función de activación Relu produce sobreajuste(muy buen acierto en train y muy malo en test) y debemos introducir dropout para balancear el ajuste\n",
    "#Cada vez que ejecuto un modelo no me salen valores iguales a pesar del random.set(seed).\n",
    "#Transfer learning mejora mucho los resultados(sobretodo con GlobalAveragePooling )\n",
    "#El earlystopping me parece un muy buen método. Detiene el sobreajuste, ahorra tiempo y recursos y produce una mayor estabilidad en el entrenamiento.\n",
    "#he aprendido que el earlystopping es ventajoso para no perder tiempo.\n",
    "#Se podría mejorar mucho más el accuracy con redes más complejas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EofHCraehRKL"
   },
   "source": [
    "Ejemplo para la tabla de resultados\n",
    "\n",
    "|Procesado de imágenes|Diseño de red neuronal|Estrategia de entrenamiento|Acierto en test|\n",
    "|---------------------|----------------------|---------------------------|---------------|\n",
    "|Tamaño 32x32, batch size 16|Convolutional(32) + Flatten + Dense(64)|Entrenamiento desde 0|xx%|\n",
    "|Tamaño 64x64, batch size 32|VGG16 + Flatten + Dense(32)|Bottleneck features|yy%|\n",
    "|...|...|...|...|"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
